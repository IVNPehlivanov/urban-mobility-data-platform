{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Basics\n",
    "This example works with RDD. The purpose of this is example is solely get to know the technology. You will probably not need RDD's in your project as we will use Spark SQL which is more flexible to use."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import datetime\n",
    "#Make sure pyspark is installed as a package of your project.\n",
    "from pyspark import SparkConf, SparkContext\n",
    "import ConnectionConfig as cc\n",
    "# This method will setup the environment variables for you. See EnvironmentSetup.py for more information.\n",
    "cc.setupEnvironment()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-05T12:28:25.253299Z",
     "start_time": "2025-02-05T12:28:25.166158Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dynamically set JAVA_HOME: /Users/user/Library/Java/JavaVirtualMachines/temurin-21.0.2/Contents/Home\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# Basic setup to be able to use a Spark Cluster"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T09:01:19.605695Z",
     "end_time": "2023-06-13T09:01:20.002796Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "#1. Creating a configuration to add parameters.\n",
    "conf = SparkConf().setAppName(\"firstJob\").setMaster(\"local[*]\").setIfMissing(\"spark.logLineage\", \"true\")\n",
    "#setMaster() is used to define on which cluster the code has to run.\n",
    "#   If you submit a job on the cluster itself the Master will be already set\n",
    "#   If you want to run the code in your development environment, you have to set the master to local[*]. Spark will initiate a local-cluster that uses the different threads of your CPU.\n",
    "# IMPORTANT: Never use setMaster(\"local[*]\") hard-coded in your final code. When running on a real environment, overwriting the default with local[*] means the job will just run locally on the master node and the cluster itself will not be used to run the job.\n",
    "\n",
    "#2. Create a sparkcontext. The context is used to initiate a processing job on the cluster\n",
    "sc =SparkContext.getOrCreate((conf))\n",
    "sc.uiWebUrl #just returns the sparkUI string. Visit the link below to get insights in the spark jobs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-05T12:28:39.400576Z",
     "start_time": "2025-02-05T12:28:35.226330Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/05 13:28:37 WARN Utils: Your hostname, MacBook-Pro-170.local resolves to a loopback address: 127.0.0.1; using 10.140.34.14 instead (on interface en0)\n",
      "25/02/05 13:28:37 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/05 13:28:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'http://10.140.34.14:4040'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Running WordCount (the Hello World of distributed processing)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T09:01:31.585604Z",
     "end_time": "2023-06-13T09:02:09.676312Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import datetime\n",
    "\n",
    "# 1. Use the Spark Context to read from an input. This is the start point for the Spark excecution engine. Reading from a textFile returns a dataframe with a record for each line.\n",
    "# TODO: Use the correct path notation (eg. file:///, gs://, hdfs://) and add this to the arguments of the job instead of hard-coded.\n",
    "lines = sc.textFile(\"./FileStore/tables/shakespeare.txt\")\\\n",
    "#2. The first transformation transforms the lines to words\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "#3. The second transformations creates a keyvalue map: map to key= word, value= 1\n",
    "wordKv = words.map(lambda word: (word, 1))\n",
    "#4. In this transformations an aggregation by key is performed. Every word is counted. Because records with the same key ar spread over the cluster, a shuffle is needed (e.g. all records with key \"romeo\" will be sent to one node). The execution engine will start a new stage for this step because of the shuffle.\n",
    "\n",
    "wordCounts = wordKv.reduceByKey(lambda a,b:a +b)\n",
    "#5. This step is not necessary but will cache the result to memory.\n",
    "# This is more efficient if the wordCounts dataframe is needed for more new dataframes to avoid that Spark recalculates everything from the beginning for each chain.\n",
    "wordCounts.cache()\n",
    "# 6. Save the counts to output. As this is an action, the Spark execution engine will only now decide that all the preceding transformations do need to be executed.\n",
    "print(wordCounts.saveAsTextFile(\"./output/words\" + datetime.datetime.now().strftime(\"%m%d%Y%H%M%S\")))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-05T12:29:05.568294Z",
     "start_time": "2025-02-05T12:28:56.430615Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/Desktop/spark_and_hadop/spark-3.5.4-bin-hadoop3/python/lib/pyspark.zip/pyspark/shuffle.py:65: UserWarning: Please install psutil to have better support with spilling\n",
      "/Users/user/Desktop/spark_and_hadop/spark-3.5.4-bin-hadoop3/python/lib/pyspark.zip/pyspark/shuffle.py:65: UserWarning: Please install psutil to have better support with spilling\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# Stops the spark context so you can build a new one.\n",
    "sc.stop()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-05T12:29:16.578966Z",
     "start_time": "2025-02-05T12:29:16.119712Z"
    }
   },
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-10T09:35:19.530280700Z",
     "start_time": "2024-09-10T09:35:19.488983100Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
