{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import datetime\n",
    "#Make sure pyspark is installed as a package of your project.\n",
    "from pyspark import SparkConf, SparkContext\n",
    "import ConnectionConfig as cc\n",
    "cc.setupEnvironment()\n",
    "conf = SparkConf().setAppName(\"firstJob\").setMaster(\"local[*]\").setIfMissing(\"spark.logLineage\", \"true\")\n",
    "sc =SparkContext.getOrCreate((conf))\n",
    "sc.uiWebUrl"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-05T12:29:33.662349Z",
     "start_time": "2025-02-05T12:29:29.287257Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dynamically set JAVA_HOME: /Users/user/Library/Java/JavaVirtualMachines/temurin-21.0.2/Contents/Home\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/05 13:29:31 WARN Utils: Your hostname, MacBook-Pro-170.local resolves to a loopback address: 127.0.0.1; using 10.140.34.14 instead (on interface en0)\n",
      "25/02/05 13:29:31 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/05 13:29:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'http://10.140.34.14:4040'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "import datetime\n",
    "\n",
    "lines = sc.textFile(\"./FileStore/tables/shakespeare.txt\")\n",
    "words = lines.flatMap(lambda line: line.split(\" \")).filter(lambda word: len(word) > 0)\n",
    "wordKv = words.map(lambda word: (word, 1))\n",
    "wordCounts = wordKv.reduceByKey(lambda a,b:a +b)\n",
    "print(wordCounts.saveAsTextFile(\"./output/words\" + datetime.datetime.now().strftime(\"%m%d%Y%H%M%S\")))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-05T12:29:42.229517Z",
     "start_time": "2025-02-05T12:29:38.007245Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/Desktop/spark_and_hadop/spark-3.5.4-bin-hadoop3/python/lib/pyspark.zip/pyspark/shuffle.py:65: UserWarning: Please install psutil to have better support with spilling\n",
      "/Users/user/Desktop/spark_and_hadop/spark-3.5.4-bin-hadoop3/python/lib/pyspark.zip/pyspark/shuffle.py:65: UserWarning: Please install psutil to have better support with spilling\n",
      "[Stage 1:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "# SparkBasicsTask\n",
    "> 1. Run the first cell to start the Spark Server\n",
    "> 2. Run the second cell to excecute the wordcount code\n",
    "> 3. Go to the SparkUI Url. How many jobs and stages are there and why?\n",
    "> 4. Copy/paste te wordcount code in a new cell.\n",
    "> 5. In the copied cell you have to add lines of code after the reduceByKey to also save the counts of first letter of the word (this way we know how many times a word starting with \"a\" appears in the text. The wordCount and letterCount must be saved seperately. Run the cell with the new code.\n",
    "_Tip: Python key values are represented by Tuples. To change the (word, count) tuple to (letter,count) you kan use a map function with (wordTuple[0][0],wordTuple[1]). The first [0] represents the first element of the tuple (the key). The second [0] is the first letter of the key string)_\n",
    "> 6. Go to the SparkUI Url. How many jobs are created? Why? Inspect the DAG of the 2 last jobs.\n",
    "_The engine creates a job for every \"action\" in the code. For every action it will create a new excecution plan and all tranformations will be processed twice._\n",
    "> 7. Go to the Storage tab. Notice the tab is empty.\n",
    "> 8. To avoid Spark from excecuting all the steps twice, add wordCounts.cache() after the wordKv.reduceByKey.\n",
    "> 9. Run the code again. How many jobs are created now? Why? Inspect the DAG of the 2 last jobs. What is the difference with the previous DAGS? Compare the jobs by opening the DAGs in two different windows. What is the difference\n",
    "> 10. Go to the Storage tab. Notice the tab is not empty anymore. Inspect the storage tab.\n",
    "> _By enabling caching, the first job will store the intermediate result. The second job will reuse this result, which will prevent processing the same steps twice. With caching, you can choose between processing and memory. When enough memory is available you can greatly increase the performance of the jobs._\n",
    "> 11. Excecute sc.stop() to stop the Spark Server"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "#Add your code here"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-10T09:38:24.274007100Z",
     "start_time": "2024-09-10T09:38:24.241385600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Execute stop after finishing the task"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "sc.stop()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-10T09:38:25.103829800Z",
     "start_time": "2024-09-10T09:38:24.255191Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
