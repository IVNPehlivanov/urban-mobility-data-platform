{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-12T10:10:39.049468Z",
     "start_time": "2025-03-12T10:09:29.140779Z"
    }
   },
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from delta import configure_spark_with_delta_pip\n",
    "import ConnectionConfigKaloyan as cc\n",
    "from ConnectionConfigKaloyan import config\n",
    "\n",
    "print(config.sections())\n",
    "cc.setupEnvironment()\n",
    "spark = cc.startLocalCluster(\"SQLExcercise\")\n",
    "spark.getActiveSession()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['default', 'tutorial_op', 'kafka']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1ef8ff1cd90>"
      ],
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-DJQSI3S:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>SQLExcercise</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "Go to https://spark.apache.org/docs/latest/sql-getting-started.html and https://spark.apache.org/docs/latest/api/python/getting_started/quickstart_df.html#Quickstart:-DataFrame to get some insights in coding Spark SQL. Always select 'Python' as the language.\n",
    "\n",
    "Use the Spark SQL Reference documentation to complete this excercise\n",
    "- To write dataframe operations: Python SparkSQL API: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/index.html\n",
    "- To write pure SQL statements: Spark SQL API: https://spark.apache.org/docs/2.3.0/api/sql/index.html and https://spark.apache.org/docs/latest/sql-ref.html\n",
    "Helpfull site with examples: https://sparkbyexamples.com/pyspark/\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load employees.csv as a Spark Dataframe\n",
    "Tip: https://spark.apache.org/docs/latest/api/python/getting_started/quickstart_df.html#Getting-Data-In/Out"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#Extract \n",
    "df = spark.read.format(\"csv\") \\\n",
    "  .option(\"header\", \"true\") \\\n",
    "  .option(\"inferSchema\", \"true\") \\\n",
    "  .load(\"./FileStore/tables/employees.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-12T10:10:55.619490Z",
     "start_time": "2025-03-12T10:10:47.083167Z"
    }
   },
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Display the schema of the DataFrame\n",
    "Tip: https://spark.apache.org/docs/latest/api/python/getting_started/quickstart_df.html#Getting-Data-In/Out"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "df.printSchema()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-12T10:11:00.119278Z",
     "start_time": "2025-03-12T10:11:00.111471Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- hire_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create a temperary view of the dataset with name tbl_employees\n",
    "https://spark.apache.org/docs/latest/api/python/getting_started/quickstart_df.html#Getting-Data-In/Out"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "df.createOrReplaceTempView(\"tbl_employees\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-12T10:11:04.943980Z",
     "start_time": "2025-03-12T10:11:04.841093Z"
    }
   },
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Calculate the total number of employees in two ways:\n",
    "-   Via dataframe operations: Tip: https://spark.apache.org/docs/latest/api/python/getting_started/quickstart_df.html#Grouping-Data\n",
    "-   With a sql statement op tbl_employees: use spark.sql()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "df.groupBy().count().show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-12T10:11:11.238894Z",
     "start_time": "2025-03-12T10:11:09.966641Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|count|\n",
      "+-----+\n",
      "|   10|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": [
    "spark.sql(\"select count(employee_id) from tbl_employees\").show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-12T10:11:13.846826Z",
     "start_time": "2025-03-12T10:11:13.095832Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|count(employee_id)|\n",
      "+------------------+\n",
      "|                10|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Find the average salary of all employees in two ways:\n",
    "-   Via the dataframe operation 'select'\n",
    "-   With a sql statement ont tbl_employees"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import *\n",
    "df.select(avg(\"salary\")).show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-12T10:11:18.402955Z",
     "start_time": "2025-03-12T10:11:18.166626Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|avg(salary)|\n",
      "+-----------+\n",
      "|     4820.0|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "source": [
    "spark.sql(\"select avg(salary) from tbl_employees\").show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-12T10:11:21.243427Z",
     "start_time": "2025-03-12T10:11:20.907599Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|avg(salary)|\n",
      "+-----------+\n",
      "|     4820.0|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Get the explain plan of the sql statement\n",
    "1. use the method explain(mode=\"extended\") on the spark.sql statement and look  at the different plans Spark created to excecute the query.\n",
    "2. Read the physical plan from bottom to top and try to match the plan with the query you wrote. (Exchange means that the data is being shuffled between the executors)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "spark.sql(\"select avg(salary) from tbl_employees\").explain(mode=\"extended\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-12T10:11:25.822142Z",
     "start_time": "2025-03-12T10:11:25.770767Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project [unresolvedalias('avg('salary), None)]\n",
      "+- 'UnresolvedRelation [tbl_employees], [], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "avg(salary): double\n",
      "Aggregate [avg(salary#20) AS avg(salary)#91]\n",
      "+- SubqueryAlias tbl_employees\n",
      "   +- View (`tbl_employees`, [employee_id#17,name#18,department#19,salary#20,hire_date#21])\n",
      "      +- Relation [employee_id#17,name#18,department#19,salary#20,hire_date#21] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [avg(salary#20) AS avg(salary)#91]\n",
      "+- Project [salary#20]\n",
      "   +- Relation [employee_id#17,name#18,department#19,salary#20,hire_date#21] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[], functions=[avg(salary#20)], output=[avg(salary)#91])\n",
      "   +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=159]\n",
      "      +- HashAggregate(keys=[], functions=[partial_avg(salary#20)], output=[sum#95, count#96L])\n",
      "         +- FileScan csv [salary#20] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/C:/Users/kkiva/data4_project_group5/examples/FileStore/tables/em..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<salary:int>\n",
      "\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Go to the SparkUI in the tab SQL/Dataframe\n",
    "1. Search for the query plans in the SparkUI SQL tab.\n",
    "2. Try to understand the excution plan.\n",
    "3. Go to https://dzone.com/articles/debugging-spark-performance-using-explain-plan to get some insights in the operators of the plan. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Find the highest salary in each department in two ways:\n",
    "-  Via the dataframe operation 'groupBy'\n",
    "-  With a sql statement ont tbl_employees"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import max\n",
    "highestSalariesByDepartment = df.groupBy(\"department\").agg(max(\"salary\").alias(\"highest_salary\"))\n",
    "highestSalariesByDepartment.show()\n",
    "\n",
    "spark.sql(f'select department, max(salary) as highest_salary from tbl_employees group by department' ).explain(mode=\"extended\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-12T10:11:33.834205Z",
     "start_time": "2025-03-12T10:11:32.090795Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+\n",
      "| department|highest_salary|\n",
      "+-----------+--------------+\n",
      "|         HR|          4800|\n",
      "|  Marketing|          4300|\n",
      "|Engineering|          6000|\n",
      "+-----------+--------------+\n",
      "\n",
      "== Parsed Logical Plan ==\n",
      "'Aggregate ['department], ['department, 'max('salary) AS highest_salary#121]\n",
      "+- 'UnresolvedRelation [tbl_employees], [], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "department: string, highest_salary: int\n",
      "Aggregate [department#19], [department#19, max(salary#20) AS highest_salary#121]\n",
      "+- SubqueryAlias tbl_employees\n",
      "   +- View (`tbl_employees`, [employee_id#17,name#18,department#19,salary#20,hire_date#21])\n",
      "      +- Relation [employee_id#17,name#18,department#19,salary#20,hire_date#21] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [department#19], [department#19, max(salary#20) AS highest_salary#121]\n",
      "+- Project [department#19, salary#20]\n",
      "   +- Relation [employee_id#17,name#18,department#19,salary#20,hire_date#21] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[department#19], functions=[max(salary#20)], output=[department#19, highest_salary#121])\n",
      "   +- Exchange hashpartitioning(department#19, 4), ENSURE_REQUIREMENTS, [plan_id=213]\n",
      "      +- HashAggregate(keys=[department#19], functions=[partial_max(salary#20)], output=[department#19, max#126])\n",
      "         +- FileScan csv [department#19,salary#20] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/C:/Users/kkiva/data4_project_group5/examples/FileStore/tables/em..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<department:string,salary:int>\n",
      "\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Calculate the total salary expenditure for each year"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "totalSalaryExpenditureByYear = df.groupBy(year(\"hire_date\").alias(\"year\")).agg(sum(\"salary\").alias(\"total_salary_expenditure\"))\n",
    "totalSalaryExpenditureByYear.show()\n",
    "\n",
    "spark.sql(f\"select year(hire_date), sum(salary) as total_salary_expenditure from tbl_employees group by year(hire_date)\").explain(mode=\"extended\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-12T10:11:38.405413Z",
     "start_time": "2025-03-12T10:11:37.849420Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------------+\n",
      "|year|total_salary_expenditure|\n",
      "+----+------------------------+\n",
      "|2019|                    8800|\n",
      "|2021|                   10300|\n",
      "|2020|                    9200|\n",
      "|2022|                    8700|\n",
      "|2018|                    6000|\n",
      "|2023|                    5200|\n",
      "+----+------------------------+\n",
      "\n",
      "== Parsed Logical Plan ==\n",
      "'Aggregate ['year('hire_date)], [unresolvedalias('year('hire_date), None), 'sum('salary) AS total_salary_expenditure#153]\n",
      "+- 'UnresolvedRelation [tbl_employees], [], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "year(hire_date): int, total_salary_expenditure: bigint\n",
      "Aggregate [year(hire_date#21)], [year(hire_date#21) AS year(hire_date)#155, sum(salary#20) AS total_salary_expenditure#153L]\n",
      "+- SubqueryAlias tbl_employees\n",
      "   +- View (`tbl_employees`, [employee_id#17,name#18,department#19,salary#20,hire_date#21])\n",
      "      +- Relation [employee_id#17,name#18,department#19,salary#20,hire_date#21] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [_groupingexpression#158], [_groupingexpression#158 AS year(hire_date)#155, sum(salary#20) AS total_salary_expenditure#153L]\n",
      "+- Project [salary#20, year(hire_date#21) AS _groupingexpression#158]\n",
      "   +- Relation [employee_id#17,name#18,department#19,salary#20,hire_date#21] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[_groupingexpression#158], functions=[sum(salary#20)], output=[year(hire_date)#155, total_salary_expenditure#153L])\n",
      "   +- Exchange hashpartitioning(_groupingexpression#158, 4), ENSURE_REQUIREMENTS, [plan_id=276]\n",
      "      +- HashAggregate(keys=[_groupingexpression#158], functions=[partial_sum(salary#20)], output=[_groupingexpression#158, sum#160L])\n",
      "         +- Project [salary#20, year(hire_date#21) AS _groupingexpression#158]\n",
      "            +- FileScan csv [salary#20,hire_date#21] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/C:/Users/kkiva/data4_project_group5/examples/FileStore/tables/em..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<salary:int,hire_date:date>\n",
      "\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Calculate the number of employees per postal code\n",
    "Postal codes are available in the parquet file empPostalCodes\n",
    "Create a view for the parquet file and join the two datasets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "df_PC =spark.read.format(\"parquet\").load(\"./FileStore/tables/empPostalCodes\")\n",
    "df_PC.createOrReplaceTempView(\"tbl_empPostalCodes\")\n",
    "df_empPerPc = spark.sql(\"select p.postal_code, count(e.employee_id) as number_of_employees from tbl_employees e inner join tbl_empPostalCodes p on e.employee_id = p.emp_id group by postal_code\")\n",
    "df_empPerPc.explain(mode=\"extended\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-12T10:11:44.407409Z",
     "start_time": "2025-03-12T10:11:43.110687Z"
    }
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/C:/Users/kkiva/data4_project_group5/examples/FileStore/tables/empPostalCodes.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mAnalysisException\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[12]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m df_PC =\u001B[43mspark\u001B[49m\u001B[43m.\u001B[49m\u001B[43mread\u001B[49m\u001B[43m.\u001B[49m\u001B[43mformat\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mparquet\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m./FileStore/tables/empPostalCodes\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m      2\u001B[39m df_PC.createOrReplaceTempView(\u001B[33m\"\u001B[39m\u001B[33mtbl_empPostalCodes\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m      3\u001B[39m df_empPerPc = spark.sql(\u001B[33m\"\u001B[39m\u001B[33mselect p.postal_code, count(e.employee_id) as number_of_employees from tbl_employees e inner join tbl_empPostalCodes p on e.employee_id = p.emp_id group by postal_code\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\data4_project_group5\\myvenv\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:307\u001B[39m, in \u001B[36mDataFrameReader.load\u001B[39m\u001B[34m(self, path, format, schema, **options)\u001B[39m\n\u001B[32m    305\u001B[39m \u001B[38;5;28mself\u001B[39m.options(**options)\n\u001B[32m    306\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(path, \u001B[38;5;28mstr\u001B[39m):\n\u001B[32m--> \u001B[39m\u001B[32m307\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._df(\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_jreader\u001B[49m\u001B[43m.\u001B[49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[32m    308\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m path \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    309\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(path) != \u001B[38;5;28mlist\u001B[39m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\data4_project_group5\\myvenv\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001B[39m, in \u001B[36mJavaMember.__call__\u001B[39m\u001B[34m(self, *args)\u001B[39m\n\u001B[32m   1316\u001B[39m command = proto.CALL_COMMAND_NAME +\\\n\u001B[32m   1317\u001B[39m     \u001B[38;5;28mself\u001B[39m.command_header +\\\n\u001B[32m   1318\u001B[39m     args_command +\\\n\u001B[32m   1319\u001B[39m     proto.END_COMMAND_PART\n\u001B[32m   1321\u001B[39m answer = \u001B[38;5;28mself\u001B[39m.gateway_client.send_command(command)\n\u001B[32m-> \u001B[39m\u001B[32m1322\u001B[39m return_value = \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1323\u001B[39m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1325\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[32m   1326\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[33m\"\u001B[39m\u001B[33m_detach\u001B[39m\u001B[33m\"\u001B[39m):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\data4_project_group5\\myvenv\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001B[39m, in \u001B[36mcapture_sql_exception.<locals>.deco\u001B[39m\u001B[34m(*a, **kw)\u001B[39m\n\u001B[32m    181\u001B[39m converted = convert_exception(e.java_exception)\n\u001B[32m    182\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[32m    183\u001B[39m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[32m    184\u001B[39m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m185\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    186\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    187\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "\u001B[31mAnalysisException\u001B[39m: [PATH_NOT_FOUND] Path does not exist: file:/C:/Users/kkiva/data4_project_group5/examples/FileStore/tables/empPostalCodes."
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Write the results to a DeltaTable in the spark-warehouse"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "df_empPerPc.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"employeesPerPostalCode\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-12T10:11:59.344941Z",
     "start_time": "2025-03-12T10:11:59.304067Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_empPerPc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[13]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[43mdf_empPerPc\u001B[49m.write.format(\u001B[33m\"\u001B[39m\u001B[33mdelta\u001B[39m\u001B[33m\"\u001B[39m).mode(\u001B[33m\"\u001B[39m\u001B[33moverwrite\u001B[39m\u001B[33m\"\u001B[39m).saveAsTable(\u001B[33m\"\u001B[39m\u001B[33memployeesPerPostalCode\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[31mNameError\u001B[39m: name 'df_empPerPc' is not defined"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "source": [
    "spark.stop()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-12T10:12:02.173738Z",
     "start_time": "2025-03-12T10:12:01.703261Z"
    }
   },
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": [
    "Stuff to create the excercises. Not part of the excercise"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "#df_PC =spark.read.format(\"csv\").option(\"header\", \"true\").load(\"./FileStore/tables/empPostalCodes.csv\")\n",
    "#df_PC.write.format(\"parquet\").mode(\"overwrite\").save(\"./FileStore/tables/empPostalCodes\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-10T09:39:52.944995500Z",
     "start_time": "2024-09-10T09:39:52.920174800Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
