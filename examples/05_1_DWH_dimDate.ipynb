{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config stuff"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T09:33:58.408429Z",
     "start_time": "2025-05-14T09:33:58.208232Z"
    }
   },
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from delta import configure_spark_with_delta_pip\n",
    "import ConnectionConfigKaloyan as cc\n",
    "cc.setupEnvironment()\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Start the cluster\n",
    "Look at the getActiveSession() method in the ConnectionConfig.py file. It will return the active session. It will also add the delta package to the session and add extra jars to the session. The jars are needed to connect to the SQL Server database."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T09:35:17.950418Z",
     "start_time": "2025-05-14T09:34:14.788326Z"
    }
   },
   "source": [
    "spark = cc.startLocalCluster(\"DIM_DATE\",1)\n",
    "spark.getActiveSession()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2182b403e10>"
      ],
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-DJQSI3S:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>DIM_DATE</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Date dimension from scratch\n",
    "\n",
    "In this example we will build a date dimension from scratch.\n",
    "\n",
    "## Step 1: Generate rows for a sequence of dates\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T09:35:33.488836Z",
     "start_time": "2025-05-14T09:35:22.183854Z"
    }
   },
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "beginDate = '2009-01-01'\n",
    "endDate = '2023-12-31'\n",
    "\n",
    "df_SQL = spark.sql(f\"select explode(sequence(to_date('{beginDate}'), to_date('{endDate}'), interval 1 day)) as calendarDate, monotonically_increasing_id() as dateSK \")\n",
    "\n",
    "\n",
    "df_SQL.createOrReplaceTempView('neededDates' )\n",
    "\n",
    "spark.sql(\"select * from neededDates\").show()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+\n",
      "|calendarDate|dateSK|\n",
      "+------------+------+\n",
      "|  2009-01-01|     0|\n",
      "|  2009-01-02|     1|\n",
      "|  2009-01-03|     2|\n",
      "|  2009-01-04|     3|\n",
      "|  2009-01-05|     4|\n",
      "|  2009-01-06|     5|\n",
      "|  2009-01-07|     6|\n",
      "|  2009-01-08|     7|\n",
      "|  2009-01-09|     8|\n",
      "|  2009-01-10|     9|\n",
      "|  2009-01-11|    10|\n",
      "|  2009-01-12|    11|\n",
      "|  2009-01-13|    12|\n",
      "|  2009-01-14|    13|\n",
      "|  2009-01-15|    14|\n",
      "|  2009-01-16|    15|\n",
      "|  2009-01-17|    16|\n",
      "|  2009-01-18|    17|\n",
      "|  2009-01-19|    18|\n",
      "|  2009-01-20|    19|\n",
      "+------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example a dataframe df_SQL is created based on the result of a select statement:\n",
    "* ```spark.sql``` is used to create date rows with sql-like language. You can find all possible SQL functions [here](https://spark.apache.org/docs/latest/api/sql/)\n",
    "*  [```sequence```](https://spark.apache.org/docs/latest/api/sql/#sequence) creates a list of dates between the begin and end date. The interval is 1 day., [```explode```](https://spark.apache.org/docs/latest/api/sql/#explode) generates a row for each item in the array.\n",
    "* [```monotonically_increasing_id```](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.monotonically_increasing_id.html#pyspark.sql.functions.monotonically_increasing_id) is used to generate a unique id in a clustered environment\n",
    "\n",
    "The dataframe is made available to use as a table called \"neededDates\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create all typical dimension fields\n",
    "Because we want to represent the date in different ways (weekday, month...), we have to do several tranformations. You can see an extract of our go\n",
    "```\n",
    "+------+--------+------------+------------+-------------+-----------+-----------+---------+--------------------+---------+----------+----------------+---------+-------------+-------------+\n",
    "|dateSK| dateInt|CalendarDate|CalendarYear|CalendarMonth|MonthOfYear|CalendarDay|DayOfWeek|DayOfWeekStartMonday|IsWeekDay|DayOfMonth|IsLastDayOfMonth|DayOfYear|WeekOfYearIso|QuarterOfYear|\n",
    "+------+--------+------------+------------+-------------+-----------+-----------+---------+--------------------+---------+----------+----------------+---------+-------------+-------------+\n",
    "|     0|20090101|  2009-01-01|        2009|      January|          1|   Thursday|        5|                   4|        Y|         1|               N|        1|            1|            1|\n",
    "|     1|20090102|  2009-01-02|        2009|      January|          1|     Friday|        6|                   5|        Y|         2|               N|        2|            1|            1|\n",
    "|     2|20090103|  2009-01-03|        2009|      January|          1|   Saturday|        7|                   6|        N|         3|               N|        3|            1|            1|\n",
    "|     3|20090104|  2009-01-04|        2009|      January|          1|     Sunday|        1|                   7|        N|         4|               N|        4|            1|            1|\n",
    "```\n",
    "\n",
    "### Method a: Use spark.sql to perform all the transformations with the help of a sql-query.\n",
    "For many, creating an SQL-select statement is the most easy way to perform the transformation."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T09:35:41.445424Z",
     "start_time": "2025-05-14T09:35:39.380778Z"
    }
   },
   "source": [
    "dimDate = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        dateSK AS date_sk,\n",
    "        CalendarDate AS date,\n",
    "        YEAR(CalendarDate) AS year,\n",
    "            CASE \n",
    "            WHEN MONTH(CalendarDate) BETWEEN 1 AND 3 THEN 1\n",
    "            WHEN MONTH(CalendarDate) BETWEEN 4 AND 6 THEN 2\n",
    "            WHEN MONTH(CalendarDate) BETWEEN 7 AND 9 THEN 3\n",
    "            ELSE 4\n",
    "            END AS quarter,\n",
    "        MONTH(CalendarDate) AS month_nr,\n",
    "        DATE_FORMAT(CalendarDate, 'MMMM') AS month_name,\n",
    "        DAYOFMONTH(CalendarDate) AS day_nr,\n",
    "        DATE_FORMAT(CalendarDate, 'EEEE') AS day_name,\n",
    "        CASE \n",
    "            WHEN WEEKDAY(CalendarDate) < 5 THEN 'Y' \n",
    "            ELSE 'N' \n",
    "        END AS is_weekday\n",
    "        \n",
    "    FROM neededDates\n",
    "    ORDER BY CalendarDate\n",
    "\"\"\")\n",
    "\n",
    "dimDate.show()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----+-------+--------+----------+------+---------+----------+\n",
      "|date_sk|      date|year|quarter|month_nr|month_name|day_nr| day_name|is_weekday|\n",
      "+-------+----------+----+-------+--------+----------+------+---------+----------+\n",
      "|      0|2009-01-01|2009|      1|       1|   January|     1| Thursday|         Y|\n",
      "|      1|2009-01-02|2009|      1|       1|   January|     2|   Friday|         Y|\n",
      "|      2|2009-01-03|2009|      1|       1|   January|     3| Saturday|         N|\n",
      "|      3|2009-01-04|2009|      1|       1|   January|     4|   Sunday|         N|\n",
      "|      4|2009-01-05|2009|      1|       1|   January|     5|   Monday|         Y|\n",
      "|      5|2009-01-06|2009|      1|       1|   January|     6|  Tuesday|         Y|\n",
      "|      6|2009-01-07|2009|      1|       1|   January|     7|Wednesday|         Y|\n",
      "|      7|2009-01-08|2009|      1|       1|   January|     8| Thursday|         Y|\n",
      "|      8|2009-01-09|2009|      1|       1|   January|     9|   Friday|         Y|\n",
      "|      9|2009-01-10|2009|      1|       1|   January|    10| Saturday|         N|\n",
      "|     10|2009-01-11|2009|      1|       1|   January|    11|   Sunday|         N|\n",
      "|     11|2009-01-12|2009|      1|       1|   January|    12|   Monday|         Y|\n",
      "|     12|2009-01-13|2009|      1|       1|   January|    13|  Tuesday|         Y|\n",
      "|     13|2009-01-14|2009|      1|       1|   January|    14|Wednesday|         Y|\n",
      "|     14|2009-01-15|2009|      1|       1|   January|    15| Thursday|         Y|\n",
      "|     15|2009-01-16|2009|      1|       1|   January|    16|   Friday|         Y|\n",
      "|     16|2009-01-17|2009|      1|       1|   January|    17| Saturday|         N|\n",
      "|     17|2009-01-18|2009|      1|       1|   January|    18|   Sunday|         N|\n",
      "|     18|2009-01-19|2009|      1|       1|   January|    19|   Monday|         Y|\n",
      "|     19|2009-01-20|2009|      1|       1|   January|    20|  Tuesday|         Y|\n",
      "+-------+----------+----+-------+--------+----------+------+---------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ```spark.sql``` is used to create the select query and returns the desired DataFrame. Remember to look-up the possible functions [here](https://spark.apache.org/docs/latest/api/sql/).\n",
    "* ```dimDate.show()``` s used to show the records in a DataFrame. Use it during development, but disable when not needed anymore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Method b: Use the dataframe API\n",
    "\n",
    "This method does not use the sql-like language. You can achieve the same with this method and you get better code completion. See [DataFrame API](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/dataframe.html)\n",
    "As an example two columns where added with   ```withColumn``` .\n",
    "```Ã¨xpr()``` is used to write a snippet of 'sql' code and parse it into a column."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T09:35:51.261895Z",
     "start_time": "2025-05-14T09:35:50.880199Z"
    }
   },
   "source": [
    "from pyspark.sql.functions import explode, expr, sequence,col, date_format\n",
    "df_SparkSQL = df_SQL \\\n",
    "    .withColumnRenamed(\"calendarDate\", 'date') \\\n",
    "    .withColumn(\"year\", date_format(\"date\",'yyyy')) \\\n",
    "    .withColumn(\"month\", date_format(\"date\",'MMMM')) \\\n",
    "    .withColumn(\"quarter\", \n",
    "                expr(\"\"\"\n",
    "                    CASE \n",
    "                        WHEN MONTH(date) BETWEEN 1 AND 3 THEN 1\n",
    "                        WHEN MONTH(date) BETWEEN 4 AND 6 THEN 2\n",
    "                        WHEN MONTH(date) BETWEEN 7 AND 9 THEN 3\n",
    "                        WHEN MONTH(date) BETWEEN 10 AND 12 THEN 4\n",
    "                    END\n",
    "                \"\"\")) \\\n",
    "    .withColumn(\"month_nr\"\n",
    "                ,expr(\"MONTH(date) AS month_nr\")) \\\n",
    "    .withColumn(\"month_name\"\n",
    "                ,date_format(\"date\",'MMMM')) \\\n",
    "    .withColumn(\"day_nr\", dayofmonth(col(\"date\"))) \\\n",
    "    .withColumn(\"day_name\", date_format(\"date\",'EEEE')) \\\n",
    "    .withColumn(\"is_weekday\",\n",
    "                expr(\"\"\"\n",
    "                    CASE \n",
    "                    WHEN WEEKDAY(date) < 5 THEN 'Y'\n",
    "                    ELSE 'N'\n",
    "                    END AS is_weekday\n",
    "                \"\"\"))\n",
    "\n",
    "df_SparkSQL.show()\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+----+-------+-------+--------+----------+------+---------+----------+\n",
      "|      date|dateSK|year|  month|quarter|month_nr|month_name|day_nr| day_name|is_weekday|\n",
      "+----------+------+----+-------+-------+--------+----------+------+---------+----------+\n",
      "|2009-01-01|     0|2009|January|      1|       1|   January|     1| Thursday|         Y|\n",
      "|2009-01-02|     1|2009|January|      1|       1|   January|     2|   Friday|         Y|\n",
      "|2009-01-03|     2|2009|January|      1|       1|   January|     3| Saturday|         N|\n",
      "|2009-01-04|     3|2009|January|      1|       1|   January|     4|   Sunday|         N|\n",
      "|2009-01-05|     4|2009|January|      1|       1|   January|     5|   Monday|         Y|\n",
      "|2009-01-06|     5|2009|January|      1|       1|   January|     6|  Tuesday|         Y|\n",
      "|2009-01-07|     6|2009|January|      1|       1|   January|     7|Wednesday|         Y|\n",
      "|2009-01-08|     7|2009|January|      1|       1|   January|     8| Thursday|         Y|\n",
      "|2009-01-09|     8|2009|January|      1|       1|   January|     9|   Friday|         Y|\n",
      "|2009-01-10|     9|2009|January|      1|       1|   January|    10| Saturday|         N|\n",
      "|2009-01-11|    10|2009|January|      1|       1|   January|    11|   Sunday|         N|\n",
      "|2009-01-12|    11|2009|January|      1|       1|   January|    12|   Monday|         Y|\n",
      "|2009-01-13|    12|2009|January|      1|       1|   January|    13|  Tuesday|         Y|\n",
      "|2009-01-14|    13|2009|January|      1|       1|   January|    14|Wednesday|         Y|\n",
      "|2009-01-15|    14|2009|January|      1|       1|   January|    15| Thursday|         Y|\n",
      "|2009-01-16|    15|2009|January|      1|       1|   January|    16|   Friday|         Y|\n",
      "|2009-01-17|    16|2009|January|      1|       1|   January|    17| Saturday|         N|\n",
      "|2009-01-18|    17|2009|January|      1|       1|   January|    18|   Sunday|         N|\n",
      "|2009-01-19|    18|2009|January|      1|       1|   January|    19|   Monday|         Y|\n",
      "|2009-01-20|    19|2009|January|      1|       1|   January|    20|  Tuesday|         Y|\n",
      "+----------+------+----+-------+-------+--------+----------+------+---------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "> ## TASK:\n",
    "> Complete the transformation in method b until the result matches the result of method a."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Writing the data to a delta-file"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T09:36:01.971286Z",
     "start_time": "2025-05-14T09:35:56.839468Z"
    }
   },
   "source": [
    "#delta files\n",
    "#dimDate.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"dimDate\")\n",
    "\n",
    "#parquet files\n",
    "dimDate.repartition(1).write.format(\"parquet\").mode(\"overwrite\").saveAsTable(\"dimDate_pq\")\n"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T16:59:17.663114Z",
     "start_time": "2025-04-22T16:59:14.994513Z"
    }
   },
   "source": [
    "spark.stop()"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-10T09:44:15.406348200Z",
     "start_time": "2024-09-10T09:44:15.371818100Z"
    }
   }
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
