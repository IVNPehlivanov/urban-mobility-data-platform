{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Config stuff"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import random\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession, functions\n",
    "import ConnectionConfig as cc\n",
    "from pyspark.sql.functions import *\n",
    "cc.setupEnvironment()\n",
    "cc.listEnvironment()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-12T12:42:50.897898Z",
     "start_time": "2025-03-12T12:42:50.890431Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HOMEBREW_PREFIX: /opt/homebrew\n",
      "COMMAND_MODE: unix2003\n",
      "INFOPATH: /opt/homebrew/share/info:\n",
      "SHELL: /bin/zsh\n",
      "PYTHONPATH: /Users/user/Desktop/data4_project_group5\n",
      "__CFBundleIdentifier: com.jetbrains.pycharm\n",
      "TMPDIR: /var/folders/k_/tkt88xx94n17f7_nvrzjrkwc0000gn/T/\n",
      "LC_ALL: en_US.UTF-8\n",
      "HOME: /Users/user\n",
      "HOMEBREW_REPOSITORY: /opt/homebrew\n",
      "PATH: /Users/user/Desktop/data4_project_group5/myenv/bin:/Users/user/Library/Java/JavaVirtualMachines/temurin-21.0.2/Contents/Home/bin:/opt/homebrew/opt/python@3.11/bin:/opt/homebrew/opt/python@3.11/bin:/opt/homebrew/bin:/opt/homebrew/opt/python@3.11/bin:/opt/homebrew/bin:/opt/homebrew/sbin:/usr/local/bin:/System/Cryptexes/App/usr/bin:/usr/bin:/bin:/usr/sbin:/sbin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/local/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/appleinternal/bin:/Users/user/Desktop/KDG/VMware Fusion.app/Contents/Public:/usr/local/go/bin:/Users/user/Desktop/spark_and_hadop/spark-3.5.4-bin-hadoop3/bin:/Users/user/Desktop/spark_and_hadop/hadoop-3.4.1-src/bin:/Users/user/Library/Java/JavaVirtualMachines/temurin-21.0.2/Contents/Home/bin:/Users/user/Desktop/spark_and_hadop/spark-3.5.4-bin-hadoop3/bin:/Users/user/Desktop/spark_and_hadop/hadoop-3.4.1-src/bin:/Users/user/Library/Java/JavaVirtualMachines/temurin-21.0.2/Contents/Home/bin:/Users/user/Desktop/spark_and_hadop/spark-3.5.4-bin-hadoop3/bin:/Users/user/Desktop/spark_and_hadop/hadoop-3.4.1-src/bin:/Users/user/Library/Java/JavaVirtualMachines/temurin-21.0.2/Contents/Home/bin:/Users/user/Desktop/spark_and_hadop/spark-3.5.4-bin-hadoop3/bin:/Users/user/Desktop/spark_and_hadop/hadoop-3.4.1-src/bin:/Users/user/Library/Java/JavaVirtualMachines/temurin-21.0.2/Contents/Home/bin\n",
      "LOGNAME: user\n",
      "XPC_FLAGS: 0x0\n",
      "__CF_USER_TEXT_ENCODING: 0x1F5:0x0:0x0\n",
      "LANG: en_US.UTF-8\n",
      "LC_CTYPE: en_US.UTF-8\n",
      "LANGUAGE: \n",
      "VIRTUAL_ENV: /Users/user/Desktop/data4_project_group5/myenv\n",
      "JAVA_HOME: /Users/user/Library/Java/JavaVirtualMachines/temurin-21.0.2/Contents/Home/\n",
      "PS1: (myenv) \n",
      "SSH_AUTH_SOCK: /private/tmp/com.apple.launchd.Sto53mD1nV/Listeners\n",
      "OLDPWD: /\n",
      "XPC_SERVICE_NAME: application.com.jetbrains.pycharm.4863247.4864111\n",
      "USER: user\n",
      "HOMEBREW_CELLAR: /opt/homebrew/Cellar\n",
      "PWD: /Users/user/Desktop/data4_project_group5\n",
      "JPY_SESSION_NAME: examples/05_2_DWH_dimSalesRep_Initial.ipynb\n",
      "JPY_PARENT_PID: 71301\n",
      "PYDEVD_USE_FRAME_EVAL: NO\n",
      "TERM: xterm-color\n",
      "CLICOLOR: 1\n",
      "FORCE_COLOR: 1\n",
      "CLICOLOR_FORCE: 1\n",
      "PAGER: cat\n",
      "GIT_PAGER: cat\n",
      "MPLBACKEND: module://matplotlib_inline.backend_inline\n",
      "PYSPARK_PYTHON: python\n",
      "SPARK_HOME: /Users/user/Desktop/spark_and_hadop/spark-3.5.4-bin-hadoop3\n",
      "HADOOP_HOME: /Users/user/Desktop/spark_and_hadop/hadoop-3.4.1-src\n",
      "PYSPARK_HADOOP_VERSION: 3\n",
      "SPARK_AUTH_SOCKET_TIMEOUT: 15\n",
      "SPARK_BUFFER_SIZE: 65536\n"
     ]
    }
   ],
   "execution_count": 102
  },
  {
   "cell_type": "code",
   "source": [
    "spark = cc.startLocalCluster(\"dimUserInitial\",4)\n",
    "spark.getActiveSession()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-12T12:42:52.818262Z",
     "start_time": "2025-03-12T12:42:52.723542Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x109e57f50>"
      ],
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.140.33.207:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>dimUserInitial</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 103
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Creating the operational database\n",
    "In order to run this demo you have to create a tutorial_op database and run the PostgreSQL_SalesOperational.sql script.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Initial load\n",
    "We will create a slowly changing dimension type 2 called dimSalesRep based on a sourceTable in our operational database called dbo.salesrep. SCD2  tables demand extra care and  because we will store hirstorical values of the dimension with the help of ranges.\n",
    "This notebook will create the table and fill it with the initial data. A second notebook will be used for increments of new and changed data.\n",
    "\n",
    "This is an example of the expected output (salesRepSK is different\n",
    "```\n",
    "+----------+-------------+-------------+-----------+-------------------+-------------------+--------------------+-------+\n",
    "|salesRepID|         name|       office| salesRepSK|          scd_start|            scd_end|                 md5|current|\n",
    "+----------+-------------+-------------+-----------+-------------------+-------------------+--------------------+-------+\n",
    "|a46add1...|      Z. Jane|     New York|          0|1990-01-01 00:00:00|2100-12-12 00:00:00|303db545462092a92...|   true|\n",
    "|s1fedf1...|   P. Chapman|       Berlin|          1|1990-01-01 00:00:00|2100-12-12 00:00:00|14b094c31bf9e4149...|   true|\n",
    "|d5e6f77...|     T. Crane|     New York|          2|1990-01-01 00:00:00|2100-12-12 00:00:00|6c062f95defda9dc3...|   true|\n",
    "```\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# Set up JDBC connection\n",
    "cc.set_connectionProfile(\"default\")\n",
    "\n",
    "df_users = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"driver\", cc.get_Property(\"driver\")) \\\n",
    "    .option(\"url\", cc.create_jdbc()) \\\n",
    "    .option(\"dbtable\", \"velo_users\") \\\n",
    "    .option(\"user\", cc.get_Property(\"username\")) \\\n",
    "    .option(\"password\", cc.get_Property(\"password\")) \\\n",
    "    .option(\"partitionColumn\", \"userid\") \\\n",
    "    .option(\"numPartitions\", 4) \\\n",
    "    .option(\"lowerBound\", 0) \\\n",
    "    .option(\"upperBound\", 20) \\\n",
    "    .load()\n",
    "\n",
    "# Create a temporary view\n",
    "df_users.createOrReplaceTempView(\"dimUser\")\n",
    "spark.sql(\"select * from dimUsers\")\n",
    "\n",
    "users_dim = spark.sql(\"select uuid() as userSK, *, to_timestamp('1999-01-01','yyyy-MM-dd') as scd_start, to_timestamp('2100-12-12','yyyy-MM-dd') as scd_end, md5(concat( street, number, zipcode, city, country_code)) as md5, True as current from dimUsers\")\n",
    "\n",
    "users_dim.printSchema()\n",
    "\n",
    "\n",
    "users_dim.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"dimUser\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-12T12:47:12.669774Z",
     "start_time": "2025-03-12T12:47:02.142160Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userSK: string (nullable = false)\n",
      " |-- userid: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- street: string (nullable = true)\n",
      " |-- number: string (nullable = true)\n",
      " |-- zipcode: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- country_code: string (nullable = true)\n",
      " |-- scd_start: timestamp (nullable = true)\n",
      " |-- scd_end: timestamp (nullable = true)\n",
      " |-- md5: string (nullable = true)\n",
      " |-- current: boolean (nullable = false)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 108
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Delete the spark session"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "spark.stop()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-12T11:23:12.093136Z",
     "start_time": "2025-03-12T11:23:11.805847Z"
    }
   },
   "outputs": [],
   "execution_count": 56
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
