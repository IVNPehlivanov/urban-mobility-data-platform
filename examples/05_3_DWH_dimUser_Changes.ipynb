{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Slowly Changing Dimension Type 2 (SCD2) - User Table\n",
    "\n",
    "## Overview\n",
    "This notebook implements an SCD2 dimension table for users (`dimuser`). The goal is to track changes in user details over time.\n",
    "\n",
    "## Steps\n",
    "1. **Load existing user dimension table (`dimuser`)** from Delta Lake.\n",
    "2. **Load latest changes** from the operational database (`velo_users`).\n",
    "3. **Detect new and changed records** using an MD5 hash comparison.\n",
    "4. **Insert new records** while keeping old versions with history.\n",
    "5. **Use Delta Lake's `MERGE INTO`** for efficient updates.\n",
    "\n",
    "## Key Fields\n",
    "| Column     | Description |\n",
    "|------------|------------|\n",
    "| `user_sk`  | Unique identifier for each user record |\n",
    "| `userid`   | Business key (same across historical records) |\n",
    "| `street`, `city`, etc. | Address details |\n",
    "| `md5`      | Hash of address details to detect changes |\n",
    "| `scd_start` | Start of record validity |\n",
    "| `scd_end`   | End of record validity |\n",
    "| `current`   | Indicates active record (TRUE) or historical (FALSE) |\n",
    "\n",
    "## Example\n",
    "If a user changes address, a new row is added while the old one is kept with an end date.\n"
   ],
   "id": "c222cb3c6a7d651b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### ðŸ“Œ Explanation\n",
    "âœ… **Import required libraries:**\n",
    "- `pyspark.sql` â†’ Provides the Spark DataFrame API.\n",
    "- `pyspark.sql.functions` â†’ Contains useful SQL functions.\n",
    "- `ConnectionConfig (cc)` â†’ Custom module to set up connections.\n",
    "\n",
    "âœ… **Set up the environment:**\n",
    "- `cc.setupEnvironment()` â†’ Configures the Spark environment.\n",
    "- `cc.listEnvironment()` â†’ Lists current environment settings.\n"
   ],
   "id": "6ed1af41ba53a042"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import ConnectionConfig as cc\n",
    "cc.setupEnvironment()\n",
    "cc.listEnvironment()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### ðŸ“Œ Explanation\n",
    "âœ… **Start a local Spark cluster:**\n",
    "- `\"dimUserChanges\"` â†’ Name of the cluster.\n",
    "- `4` â†’ Number of worker threads.\n",
    "\n",
    "âœ… **Get the active Spark session:**\n",
    "- `getActiveSession()` â†’ Ensures the session is running."
   ],
   "id": "836c66dd7fb3c0c2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "spark = cc.startLocalCluster(\"dimUserChanges\",4)\n",
    "spark.getActiveSession()"
   ],
   "id": "cd163f56a304abac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "45e940744ebbaeb9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### ðŸ“Œ Explanation\n",
    "âœ… **Capture the job execution timestamp:**\n",
    "- `datetime.now()` â†’ Retrieves the current timestamp.\n",
    "- This timestamp will be used to track when records were processed."
   ],
   "id": "f623c5c7692bb3d3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from datetime import *\n",
    "run_timestamp =datetime.now() #The job runtime is stored in a variable\n",
    "print(run_timestamp)"
   ],
   "id": "5cb726f6775aadfa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### ðŸ“Œ Explanation\n",
    "âœ… **Load the existing dimension table:**\n",
    "- `DeltaTable.forPath(...)` â†’ Loads the `dimuser` table from Delta Lake.\n",
    "\n",
    "âœ… **Create a temporary SQL view:**\n",
    "- `createOrReplaceTempView(\"dim_users_current\")` â†’ Allows querying via SQL.\n",
    "\n",
    "âœ… **Show existing records:**\n",
    "- `spark.sql(\"SELECT * FROM dim_users_current\").show()` â†’ Displays data.\n"
   ],
   "id": "a92de7007ef5d452"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from delta import DeltaTable\n",
    "current_user_table = DeltaTable.forPath(spark, \"./spark-warehouse/dimuser\")"
   ],
   "id": "e13990f3d7fd5936",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "current_user_table.toDF().createOrReplaceTempView(\"dim_users_current\")",
   "id": "bacab5f49d9382d3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "spark.sql(\"select * from dim_users_current\").show()",
   "id": "9abb2a1d57dbea0d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### ðŸ“Œ Explanation\n",
    "âœ… **Read new user data from the operational database:**\n",
    "- Uses **JDBC** to connect to the database.\n",
    "\n",
    "âœ… **Create a temporary SQL view:**\n",
    "- `createOrReplaceTempView(\"users_operational_db\")` â†’ Enables SQL queries on the latest user data.\n"
   ],
   "id": "18eddf1af67d3674"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "### LOAD THE NEWEST CHANGES FROM THE OPERATIONAL DATABASE\n",
    "df_users = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"driver\", cc.get_Property(\"driver\")) \\\n",
    "    .option(\"url\", cc.create_jdbc()) \\\n",
    "    .option(\"dbtable\", \"velo_users\") \\\n",
    "    .option(\"user\", cc.get_Property(\"username\")) \\\n",
    "    .option(\"password\", cc.get_Property(\"password\")) \\\n",
    "    .load()\n",
    "\n",
    "df_users.createOrReplaceTempView(\"users_operational_db\")"
   ],
   "id": "1373a586d36b2f01",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### ðŸ“Œ Explanation\n",
    "âœ… **Transform the operational data into a dimension format:**\n",
    "- `uuid()` â†’ Generates a **unique key** for each record.\n",
    "- `md5(...)` â†’ Creates a **hash of address details** to detect changes.\n",
    "\n",
    "âœ… **Create a SQL view for transformed data:**\n",
    "- `createOrReplaceTempView(\"dim_users_new\")` â†’ Allows comparison with existing data.\n",
    "\n",
    "âœ… **Show transformed records:**\n",
    "- `dim_users_new.show()` â†’ Displays the new data.\n"
   ],
   "id": "9db0ffdc69939368"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "### TRANSFORM THE SOURCE TABLE TO the dimension format\n",
    "### IMPORTANT !!!!!!!!! CHECKING ONLY THE STREET FOR NOW !!!!!!!!\n",
    "dim_users_new = spark.sql( \"select uuid() as source_user_sk, \\\n",
    "                                        userid as source_userid, \\\n",
    "                                        name as source_name, \\\n",
    "                                        street as source_street, \\\n",
    "                                        md5(concat( street, number, zipcode, city, country_code)) as source_md5 \\\n",
    "                                    from users_operational\")\n",
    "dim_users_new.createOrReplaceTempView(\"dim_users_new\")"
   ],
   "id": "eff159cacfe05040",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "dim_users_new.show()",
   "id": "85905e2838aa82b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### ðŸ“Œ Explanation\n",
    "âœ… **Detect changes:**\n",
    "- `LEFT OUTER JOIN` â†’ Compare new records (`dim_users_new`) with the existing ones (`dim_users_current`).\n",
    "- `WHERE dwh.userid IS NULL` â†’ Identifies **new users**.\n",
    "- `OR dwh.md5 <> source.source_md5` â†’ Identifies **modified users**.\n"
   ],
   "id": "22ef2a6451c44fbc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "detectedChanges=spark.sql(f\"select * \\\n",
    "                          from dim_users_new as source \\\n",
    "                          left outer join dim_users_current as dwh on dwh.userid == source.source_userid and dwh.current == true \\\n",
    "                          where dwh.userid is null or dwh.md5 <> source.source_md5\")\n",
    "\n",
    "detectedChanges.createOrReplaceTempView(\"detectedChanges\")"
   ],
   "id": "df04a490fef7376a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#DEBUG CODE TO SHOW CONTENT OF DETECTED CHANGES\n",
    "detectedChanges.show()"
   ],
   "id": "43fd4080b3a35a7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### ðŸ“Œ Explanation\n",
    "âœ… **Prepare data for update and insert:**\n",
    "- Inserts **new records**.\n",
    "- Updates **existing records** (sets `scd_end` and `current = FALSE`).\n"
   ],
   "id": "c1fcf5e7bd0a6f53"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "upserts = spark.sql(f\"select source_user_sk as user_sk,\\\n",
    "                                source_userid as userid,\\\n",
    "                                source_name as name,\\\n",
    "                                source_street as street,\\\n",
    "                                to_timestamp('{run_timestamp}') as scd_start, \\\n",
    "                                to_timestamp('2100-12-12','yyyy-MM-dd') as scd_end,\\\n",
    "                                source_md5 as md5,\\\n",
    "                                true as current\\\n",
    "                        from  detectedChanges\\\n",
    "                        union \\\n",
    "                        select  userSK,\\\n",
    "                                userid,\\\n",
    "                                name,\\\n",
    "                                street,\\\n",
    "                                scd_start,\\\n",
    "                                to_timestamp('{run_timestamp}') as scd_end,\\\n",
    "                                md5, \\\n",
    "                                false \\\n",
    "                                from detectedChanges \\\n",
    "                        where current is not null\")\n",
    "\n",
    "upserts.createOrReplaceTempView(\"upserts\")"
   ],
   "id": "9412d207514c6e6d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#DEBUG CODE TO SHOW CONTENT OF UPSERTS\n",
    "spark.sql(\"select * from upserts\").show()"
   ],
   "id": "e25852101e820065",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### ðŸ“Œ Explanation\n",
    "âœ… **Perform the SCD2 merge operation:**\n",
    "- **UPDATE** existing records â†’ Sets `scd_end` and `current = FALSE`.\n",
    "- **INSERT** new records â†’ Tracks changes as new entries.\n"
   ],
   "id": "874f80f6eb799e4d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "spark.sql(\"MERGE INTO dim_users_current AS target \\\n",
    "          using upserts AS source ON target.userid = source.userid and source.current = false and target.current=true \\\n",
    "          WHEN MATCHED THEN UPDATE SET scd_end = source.scd_end, current = source.current  \\\n",
    "          WHEN NOT MATCHED THEN INSERT (userSK, userid, name, street, scd_start, scd_end, md5, current) values (source.user_sk, source.userid, source.name, source.street, source.scd_start, source.scd_end, source.md5, source.current)\")\n"
   ],
   "id": "dab53b5b74fc1334",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### ðŸ“Œ Explanation\n",
    "âœ… **Display the final dimension table sorted by `userid` and `scd_start`**  \n",
    "- Shows **historical versions** and the **latest active record**.\n"
   ],
   "id": "327402f0b3bc19e1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "current_user_table.toDF().sort(\"userid\", \"scd_start\").show(100)",
   "id": "a81235789f091db1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "spark.stop()",
   "id": "1e62a6b551939ee5",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
