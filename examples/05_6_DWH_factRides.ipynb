{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Config stuff"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "from xmlrpc.client import DateTime\n",
    "\n",
    "import ConnectionConfig as cc\n",
    "from delta import DeltaTable\n",
    "cc.setupEnvironment()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-23T09:05:52.539442Z",
     "start_time": "2025-04-23T09:05:52.405021Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dynamically set JAVA_HOME: /Users/user/Library/Java/JavaVirtualMachines/temurin-21.0.2/Contents/Home\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": [
    "spark = cc.startLocalCluster(\"FACT_RIDE\")\n",
    "spark.getActiveSession()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-23T09:06:01.204336Z",
     "start_time": "2025-04-23T09:05:53.349613Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/user/Desktop/spark_and_hadop/spark-3.5.4-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/user/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/user/.ivy2/jars\n",
      "io.delta#delta-spark_2.12 added as a dependency\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      "org.postgresql#postgresql added as a dependency\n",
      "org.elasticsearch#elasticsearch-spark-30_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-8183ec6c-7e3e-42a4-8237-c625574535a3;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.12;3.2.0 in central\n",
      "\tfound io.delta#delta-storage;3.2.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.4.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.4.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.3.2 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.9.1 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.6 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      "\tfound org.postgresql#postgresql;42.7.4 in central\n",
      "\tfound org.checkerframework#checker-qual;3.42.0 in central\n",
      "\tfound org.elasticsearch#elasticsearch-spark-30_2.12;8.15.2 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.12.19 in central\n",
      "\tfound javax.xml.bind#jaxb-api;2.3.1 in central\n",
      "\tfound org.apache.spark#spark-yarn_2.12;3.4.3 in central\n",
      ":: resolution report :: resolve 1031ms :: artifacts dl 24ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\tio.delta#delta-spark_2.12;3.2.0 from central in [default]\n",
      "\tio.delta#delta-storage;3.2.0 from central in [default]\n",
      "\tjavax.xml.bind#jaxb-api;2.3.1 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.3.2 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.4.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.4.0 from central in [default]\n",
      "\torg.apache.spark#spark-yarn_2.12;3.4.3 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.42.0 from central in [default]\n",
      "\torg.elasticsearch#elasticsearch-spark-30_2.12;8.15.2 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.postgresql#postgresql;42.7.4 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.19 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.6 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.9.1 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcommons-logging#commons-logging;1.1.1 by [commons-logging#commons-logging;1.1.3] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   21  |   0   |   0   |   1   ||   20  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-8183ec6c-7e3e-42a4-8237-c625574535a3\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 20 already retrieved (0kB/43ms)\n",
      "25/04/23 11:05:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x11015ef10>"
      ],
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.140.33.70:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>FACT_RIDE</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Fact transformations\n",
    "This notebooks creates the sales fact table from scratch based on the operational source table \"sales\"\n",
    "When creating a fact table always follow the listed steps in order."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "#### 1 READ NECESSARY SOURCE TABLE(S) AND PERFORM TRANSFORMATIONS\n",
    "**When reading from the source table make sure you include all data necessary:**\n",
    "- to calculate the measure values\n",
    "- the source table keys that you have to use to lookup the correct surrogate keys in the dimension tables.\n",
    "\n",
    "**If more than one table is needed to gather the necesary information you can opt for one of two strategies:**\n",
    "- Use a select query when reading from the jdbc source with the spark.read operation. Avoid complex queries because the operational database needs a lot of resources to run those queries.\n",
    "- Perform a spark.read operation for each table separately and join the tables within Spark. The joins will take place on the cluster instead of the database. You limit the database recources used, but there can be a significant overhead of unnecessary data tranferred to the cluster.\n",
    "\n",
    "\n",
    "In this case we just rename Amount and create a default count_mv column.\n",
    "The transformations are minimal. In reality, transformations can be far more complex. If so, it can be advisable to work out the transforms in more then one step.*\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "cc.set_connectionProfile(\"default\")\n",
    "ride_src_df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", cc.create_jdbc()) \\\n",
    "    .option(\"driver\" , cc.get_Property(\"driver\")) \\\n",
    "    .option(\"dbtable\",\"rides\").option(\"user\", cc.get_Property(\"username\")) \\\n",
    "    .option(\"password\", cc.get_Property(\"password\")) \\\n",
    "    .option(\"partitionColumn\", \"rideid\") \\\n",
    "    .option(\"numPartitions\", 4) \\\n",
    "    .option(\"lowerBound\", 0) \\\n",
    "    .option(\"upperBound\", 1000) \\\n",
    "    .load()\n",
    "\n",
    "subscriptions_df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", cc.create_jdbc()) \\\n",
    "    .option(\"driver\" , cc.get_Property(\"driver\")) \\\n",
    "    .option(\"dbtable\",\"subscriptions\").option(\"user\", cc.get_Property(\"username\")) \\\n",
    "    .option(\"password\", cc.get_Property(\"password\")) \\\n",
    "    .option(\"partitionColumn\", \"subscriptionid\") \\\n",
    "    .option(\"numPartitions\", 4) \\\n",
    "    .option(\"lowerBound\", 0) \\\n",
    "    .option(\"upperBound\", 1000) \\\n",
    "    .load()\n",
    "\n",
    "vehicles_df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", cc.create_jdbc()) \\\n",
    "    .option(\"driver\" , cc.get_Property(\"driver\")) \\\n",
    "    .option(\"dbtable\",\"vehicles\").option(\"user\", cc.get_Property(\"username\")) \\\n",
    "    .option(\"password\", cc.get_Property(\"password\")) \\\n",
    "    .option(\"partitionColumn\", \"vehicleid\") \\\n",
    "    .option(\"numPartitions\", 4) \\\n",
    "    .option(\"lowerBound\", 0) \\\n",
    "    .option(\"upperBound\", 1000) \\\n",
    "    .load()\n",
    "\n",
    "bikelots_df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", cc.create_jdbc()) \\\n",
    "    .option(\"driver\" , cc.get_Property(\"driver\")) \\\n",
    "    .option(\"dbtable\",\"bikelots\").option(\"user\", cc.get_Property(\"username\")) \\\n",
    "    .option(\"password\", cc.get_Property(\"password\")) \\\n",
    "    .option(\"partitionColumn\", \"bikelotid\") \\\n",
    "    .option(\"numPartitions\", 4) \\\n",
    "    .option(\"lowerBound\", 0) \\\n",
    "    .option(\"upperBound\", 1000) \\\n",
    "    .load()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# weather_df = spark.read.json(r'C:\\Users\\kkiva\\data4_project_group5\\examples\\weather\\*.json')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-23T09:06:05.597434Z",
     "start_time": "2025-04-23T09:06:04.129241Z"
    }
   },
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T09:22:20.326792Z",
     "start_time": "2025-04-23T09:22:20.317861Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, IntegerType, ArrayType, DateType\n",
    "\n",
    "# Define the schema for the JSON structure based on the provided response\n",
    "weather_schema = StructType([\n",
    "    StructField(\"zipCode\", StringType(), True),\n",
    "    StructField(\"coord\", StructType([\n",
    "        StructField(\"lon\", FloatType(), True),\n",
    "        StructField(\"lat\", FloatType(), True)\n",
    "    ]), True),\n",
    "    StructField(\"weather\", ArrayType(StructType([\n",
    "        StructField(\"id\", IntegerType(), True),\n",
    "        StructField(\"main\", StringType(), True),\n",
    "        StructField(\"description\", StringType(), True),\n",
    "        StructField(\"icon\", StringType(), True)\n",
    "    ])), True),\n",
    "    StructField(\"base\", StringType(), True),\n",
    "    StructField(\"main\", StructType([\n",
    "        StructField(\"temp\", FloatType(), True),\n",
    "        StructField(\"feels_like\", FloatType(), True),\n",
    "        StructField(\"temp_min\", FloatType(), True),\n",
    "        StructField(\"temp_max\", FloatType(), True),\n",
    "        StructField(\"pressure\", IntegerType(), True),\n",
    "        StructField(\"humidity\", IntegerType(), True),\n",
    "        StructField(\"sea_level\", IntegerType(), True),\n",
    "        StructField(\"grnd_level\", IntegerType(), True)\n",
    "    ]), True),\n",
    "    StructField(\"visibility\", IntegerType(), True),\n",
    "    StructField(\"wind\", StructType([\n",
    "        StructField(\"speed\", FloatType(), True),\n",
    "        StructField(\"deg\", IntegerType(), True),\n",
    "        StructField(\"gust\", FloatType(), True)\n",
    "    ]), True),\n",
    "    StructField(\"rain\", StructType([\n",
    "        StructField(\"1h\", FloatType(), True)\n",
    "    ]), True),\n",
    "    StructField(\"clouds\", StructType([\n",
    "        StructField(\"all\", IntegerType(), True)\n",
    "    ]), True),\n",
    "    StructField(\"dt\", IntegerType(), True),\n",
    "    StructField(\"api_current_time\", StringType(), True),\n",
    "    StructField(\"sys\", StructType([\n",
    "        StructField(\"type\", IntegerType(), True),\n",
    "        StructField(\"id\", IntegerType(), True),\n",
    "        StructField(\"country\", StringType(), True),\n",
    "        StructField(\"sunrise\", IntegerType(), True),\n",
    "        StructField(\"sunset\", IntegerType(), True)\n",
    "    ]), True),\n",
    "    StructField(\"timezone\", IntegerType(), True),\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"cod\", IntegerType(), True)\n",
    "])\n"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T16:34:33.811660Z",
     "start_time": "2025-03-29T16:34:33.790039Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'StructType' object has no attribute 'createOrReplaceTempView'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mAttributeError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[15]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[43mweather_schema\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcreateOrReplaceTempView\u001B[49m(\u001B[33m\"\u001B[39m\u001B[33mweatherdimx\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[31mAttributeError\u001B[39m: 'StructType' object has no attribute 'createOrReplaceTempView'"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T09:22:29.760696Z",
     "start_time": "2025-04-23T09:22:28.726816Z"
    }
   },
   "cell_type": "code",
   "source": [
    "weather_df = spark.read.option(\"multiline\", \"true\").schema(weather_schema).json(\"weather/*.json\")\n",
    "weather_df.printSchema()\n",
    "weather_df.show(5, truncate=False)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- zipCode: string (nullable = true)\n",
      " |-- coord: struct (nullable = true)\n",
      " |    |-- lon: float (nullable = true)\n",
      " |    |-- lat: float (nullable = true)\n",
      " |-- weather: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- id: integer (nullable = true)\n",
      " |    |    |-- main: string (nullable = true)\n",
      " |    |    |-- description: string (nullable = true)\n",
      " |    |    |-- icon: string (nullable = true)\n",
      " |-- base: string (nullable = true)\n",
      " |-- main: struct (nullable = true)\n",
      " |    |-- temp: float (nullable = true)\n",
      " |    |-- feels_like: float (nullable = true)\n",
      " |    |-- temp_min: float (nullable = true)\n",
      " |    |-- temp_max: float (nullable = true)\n",
      " |    |-- pressure: integer (nullable = true)\n",
      " |    |-- humidity: integer (nullable = true)\n",
      " |    |-- sea_level: integer (nullable = true)\n",
      " |    |-- grnd_level: integer (nullable = true)\n",
      " |-- visibility: integer (nullable = true)\n",
      " |-- wind: struct (nullable = true)\n",
      " |    |-- speed: float (nullable = true)\n",
      " |    |-- deg: integer (nullable = true)\n",
      " |    |-- gust: float (nullable = true)\n",
      " |-- rain: struct (nullable = true)\n",
      " |    |-- 1h: float (nullable = true)\n",
      " |-- clouds: struct (nullable = true)\n",
      " |    |-- all: integer (nullable = true)\n",
      " |-- dt: integer (nullable = true)\n",
      " |-- api_current_time: string (nullable = true)\n",
      " |-- sys: struct (nullable = true)\n",
      " |    |-- type: integer (nullable = true)\n",
      " |    |-- id: integer (nullable = true)\n",
      " |    |-- country: string (nullable = true)\n",
      " |    |-- sunrise: integer (nullable = true)\n",
      " |    |-- sunset: integer (nullable = true)\n",
      " |-- timezone: integer (nullable = true)\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- cod: integer (nullable = true)\n",
      "\n",
      "+-------+------------------+------------------------------------+--------+---------------------------------------------+----------+-----------------+-----+------+----------+----------------+----------------------------------------+--------+------+-----+---+\n",
      "|zipCode|coord             |weather                             |base    |main                                         |visibility|wind             |rain |clouds|dt        |api_current_time|sys                                     |timezone|id    |name |cod|\n",
      "+-------+------------------+------------------------------------+--------+---------------------------------------------+----------+-----------------+-----+------+----------+----------------+----------------------------------------+--------+------+-----+---+\n",
      "|2018   |{4.42545, 51.2037}|[{3, Neutral, neutral weather, 03d}]|stations|{12.6, 12.6, 11.6, 14.1, 1015, 50, 1015, 933}|10000     |{12.2, 109, 12.7}|{0.0}|{0}   |1742295600|2025-04-23T11:00|{2, 2075663, BE, 1742284800, 1742328000}|3600    |201800|Zocca|200|\n",
      "|2018   |{4.42545, 51.2037}|[{3, Neutral, neutral weather, 03d}]|stations|{12.6, 12.6, 11.6, 14.1, 1015, 50, 1015, 933}|10000     |{12.2, 109, 12.7}|{0.0}|{0}   |1742299200|2025-04-23T11:00|{2, 2075663, BE, 1742288400, 1742331600}|3600    |201800|Zocca|200|\n",
      "|2018   |{4.42545, 51.2037}|[{3, Neutral, neutral weather, 03d}]|stations|{12.6, 12.6, 11.6, 14.1, 1015, 50, 1015, 933}|10000     |{12.2, 109, 12.7}|{0.0}|{0}   |1742302800|2025-04-23T11:00|{2, 2075663, BE, 1742292000, 1742335200}|3600    |201800|Zocca|200|\n",
      "|2020   |{4.39731, 51.1894}|[{3, Neutral, neutral weather, 03d}]|stations|{12.6, 12.6, 11.6, 14.1, 1015, 50, 1015, 933}|10000     |{12.6, 110, 13.1}|{0.0}|{0}   |1742295600|2025-04-23T11:00|{2, 2075663, BE, 1742284800, 1742328000}|3600    |202000|Zocca|200|\n",
      "|2020   |{4.39731, 51.1894}|[{3, Neutral, neutral weather, 03d}]|stations|{12.6, 12.6, 11.6, 14.1, 1015, 50, 1015, 933}|10000     |{12.6, 110, 13.1}|{0.0}|{0}   |1742299200|2025-04-23T11:00|{2, 2075663, BE, 1742288400, 1742331600}|3600    |202000|Zocca|200|\n",
      "+-------+------------------+------------------------------------+--------+---------------------------------------------+----------+-----------------+-----+------+----------+----------------+----------------------------------------+--------+------+-----+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "#### 2 MAKE DIMENSION TABLES AVAILABLE AS VIEWS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "dim_date = spark.read.format(\"delta\").load(\"spark-warehouse/dimdate\")\n",
    "dim_vehicle = spark.read.format(\"delta\").load(\"spark-warehouse/dimvehicle\")\n",
    "dim_user = spark.read.format(\"delta\").load(\"spark-warehouse/dimuser\")\n",
    "dim_weather = spark.read.format(\"delta\").load(\"spark-warehouse/dimweather\")\n",
    "dim_lock = spark.read.format(\"delta\").load(\"spark-warehouse/dimlock\")\n",
    "\n",
    "dim_date.createOrReplaceTempView(\"dimDate\")\n",
    "dim_user.createOrReplaceTempView(\"dimUser\")\n",
    "dim_vehicle.createOrReplaceTempView(\"dimVehicle\")\n",
    "dim_weather.createOrReplaceTempView(\"dimWeather\")\n",
    "dim_lock.createOrReplaceTempView(\"dimLock\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-23T09:22:32.572126Z",
     "start_time": "2025-04-23T09:22:32.415094Z"
    }
   },
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T08:35:08.850071Z",
     "start_time": "2025-04-23T08:35:05.094414Z"
    }
   },
   "cell_type": "code",
   "source": [
    "xxd = spark.sql(\"\"\"\n",
    "SELECT *  FROM dimVehicle;\n",
    "\"\"\")\n",
    "xxd.show()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-------------------+\n",
      "|vehicleid|biketypeid|biketypedescription|\n",
      "+---------+----------+-------------------+\n",
      "|     NULL|         1|          Velo Bike|\n",
      "|     NULL|         2|        Velo E-Bike|\n",
      "|     NULL|         3|               Step|\n",
      "|     NULL|         4|            Scooter|\n",
      "+---------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "#### 3 Build the fact table\n",
    "\n",
    "Within the creation of a fact table always perform these two tasks:\n",
    "1.   Include the measures of the fact\n",
    "2. Use the dimension tables to look up the surrogate keys that correspond with the natural key value. In case of SCD2 dimension use the scd_start en scd_end to find the correct version of the data in the dimension\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T09:22:34.997840Z",
     "start_time": "2025-04-23T09:22:34.963042Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "import math\n",
    "\n",
    "def haversine_km(lat1, lon1, lat2, lon2):\n",
    "    if None in (lat1, lon1, lat2, lon2):  # Handle NULL values safely\n",
    "        return None  \n",
    "    R = 6371  # Radius of Earth in km\n",
    "    lat1, lon1, lat2, lon2 = map(math.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = math.sin(dlat / 2) ** 2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2) ** 2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    return R * c\n",
    "\n",
    "# Register the UDF in Spark SQL\n",
    "haversine_udf = udf(haversine_km, DoubleType())\n",
    "spark.udf.register(\"haversine_km\", haversine_udf)\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/23 11:22:34 WARN SimpleFunctionRegistry: The function haversine_km replaced a previously registered function.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.udf.UserDefinedFunction at 0x1127b2f10>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "source": [
    "ride_src_df.createOrReplaceTempView(\"rides_source\")\n",
    "subscriptions_df.createOrReplaceTempView(\"subscriptions_source\")\n",
    "\n",
    "# Create temp views for the required dataframes\n",
    "weather_df.createOrReplaceTempView(\"weather_data\")\n",
    "\n",
    "vehicles_df.createOrReplaceTempView(\"vehicle_data\")\n",
    "bikelots_df.createOrReplaceTempView(\"bikelots_data\")\n",
    "\n",
    "ridesFactFromSource = spark.sql(\"\"\"\n",
    "    SELECT src.rideid AS ride_id, \n",
    "           du.userSK AS user_sk, \n",
    "           src.startlockid AS start_lock_id, \n",
    "           src.endlockid AS end_lock_id, \n",
    "           dd.date_sk AS date_sk, \n",
    "           dv.biketypeid AS vehicle_id, \n",
    "              CASE \n",
    "           WHEN unix_timestamp(src.endtime) > unix_timestamp(src.starttime) \n",
    "           THEN ROUND((unix_timestamp(src.endtime) - unix_timestamp(src.starttime)) / 60, 2)\n",
    "           ELSE 0  \n",
    "           END AS ride_duration, \n",
    "           Round(haversine_km(\n",
    "               CAST(SPLIT(REPLACE(dl_start.gpscoord, '(', ''), ',')[0] AS DOUBLE),\n",
    "               CAST(SPLIT(REPLACE(REPLACE(dl_start.gpscoord, ')', ''), '(', ''), ',')[1] AS DOUBLE),\n",
    "               CAST(SPLIT(REPLACE(dl_end.gpscoord, '(', ''), ',')[0] AS DOUBLE),\n",
    "               CAST(SPLIT(REPLACE(REPLACE(dl_end.gpscoord, ')', ''), '(', ''), ',')[1] AS DOUBLE)\n",
    "           ),2) AS distance_km,\n",
    "           MIN(dw.weather_id) AS weather_id,  \n",
    "           md5(concat(src.rideid, du.userSK, src.startlockid, src.endlockid, dd.date_sk, dv.biketypeid)) AS md5 \n",
    "    FROM rides_source AS src \n",
    "    LEFT OUTER JOIN subscriptions_source AS sub ON src.subscriptionid = sub.subscriptionid \n",
    "    LEFT OUTER JOIN dimUser AS du ON sub.userid = du.userid \n",
    "    LEFT OUTER JOIN dimLock AS dl_start ON src.startlockid = dl_start.lockid  \n",
    "    LEFT OUTER JOIN dimLock AS dl_end ON src.endlockid = dl_end.lockid  \n",
    "    LEFT OUTER JOIN dimDate AS dd ON DATE(src.starttime) = dd.date \n",
    "    LEFT OUTER JOIN vehicle_data AS vd ON src.vehicleid = vd.vehicleid\n",
    "    LEFT OUTER JOIN bikelots_data AS bl ON vd.bikelotid = bl.bikelotid\n",
    "    LEFT OUTER JOIN dimVehicle as dv ON bl.biketypeid = dv.biketypeid\n",
    "    LEFT OUTER JOIN weather_data AS wd ON dl_start.zipcode = wd.zipcode\n",
    "    LEFT OUTER JOIN dimWeather AS dw ON wd.weather[0].main = dw.weather_condition\n",
    "    GROUP BY src.rideid, du.userSK, src.startlockid, src.endlockid, dd.date_sk, dv.biketypeid,\n",
    "             src.starttime, src.endtime, dl_start.gpscoord, dl_end.gpscoord\n",
    "\"\"\")\n"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T09:25:47.779114Z",
     "start_time": "2025-04-23T09:25:47.588850Z"
    }
   },
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T09:23:57.192738Z",
     "start_time": "2025-04-23T09:23:57.109305Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ride_src_df.createOrReplaceTempView(\"rides_source\")\n",
    "subscriptions_df.createOrReplaceTempView(\"subscriptions_source\")"
   ],
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T09:19:04.627623Z",
     "start_time": "2025-04-23T09:19:04.018004Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# trying to fix null subscription ids \n",
    "xd = spark.sql(\"\"\"\n",
    "SELECT * from rides_source\n",
    "\n",
    "\n",
    "\"\"\")\n",
    "xd.show()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------+-----------------+-------------------+-------------------+---------+--------------+-----------+---------+\n",
      "|rideid|       startpoint|         endpoint|          starttime|            endtime|vehicleid|subscriptionid|startlockid|endlockid|\n",
      "+------+-----------------+-----------------+-------------------+-------------------+---------+--------------+-----------+---------+\n",
      "|     1|(51.2083,4.44595)|(51.1938,4.40228)|2015-09-22 00:00:00|2012-09-22 00:00:00|      844|         13296|       4849|     3188|\n",
      "|     2|(51.2174,4.41597)|(51.2188,4.40935)|2015-09-22 00:00:00|2012-09-22 00:00:00|     4545|         45924|       NULL|     NULL|\n",
      "|     3|(51.2088,4.40834)|(51.2077,4.39846)|2015-09-22 00:00:00|2012-09-22 00:00:00|     3419|         25722|       2046|     1951|\n",
      "|     4|(51.2023,4.41208)|(51.2119,4.39894)|2015-09-22 00:00:00|2012-09-22 00:00:00|     1208|         31000|       1821|     2186|\n",
      "|     5|(51.1888,4.45039)|(51.2221,4.40467)|2015-09-22 00:00:00|2012-09-22 00:00:00|     5536|         59732|       6382|     2700|\n",
      "|     6|(51.2159,4.41073)|(51.2191,4.41596)|2015-09-22 00:00:00|2012-09-22 00:00:00|     6336|          NULL|       NULL|     NULL|\n",
      "|     7|(51.2038,4.42077)|(51.2055,4.39933)|2015-09-22 00:00:00|2012-09-22 00:00:00|     4045|         31055|       1388|     3401|\n",
      "|     8| (51.2191,4.3949)|  (51.2281,4.409)|2015-09-22 00:00:00|2012-09-22 00:00:00|     2015|         65164|       2572|       13|\n",
      "|     9|(51.2182,4.41238)| (51.209,4.43266)|2015-09-22 00:00:00|2012-09-22 00:00:00|     5298|         71164|         50|     2067|\n",
      "|    10|(51.2196,4.41724)|(51.2143,4.41316)|2015-09-22 00:00:00|2012-09-22 00:00:00|     6332|         68426|       NULL|     NULL|\n",
      "|    11|(51.2179,4.41777)| (51.211,4.40724)|2015-09-22 00:00:00|2012-09-22 00:00:00|     1400|           999|        985|     2148|\n",
      "|    12|(51.2088,4.40834)|(51.2145,4.44373)|2015-09-22 00:00:00|2012-09-22 00:00:00|      957|         59847|       2039|     3038|\n",
      "|    13|(51.1744,4.40334)|(51.2228,4.42426)|2015-09-22 00:00:00|2012-09-22 00:00:00|     5413|          5045|       5619|     2717|\n",
      "|    14|   (51.25,4.4209)|   (51.25,4.4209)|2015-09-22 00:00:00|2012-09-22 00:00:00|     2658|         65816|       3531|     3554|\n",
      "|    15|(51.2083,4.44595)|(51.1938,4.40228)|2019-09-22 08:46:43|2019-09-22 09:01:36|      844|         13296|       4849|     3188|\n",
      "|    16|(51.2174,4.41597)|(51.2188,4.40935)|2019-09-22 08:19:51|2019-09-22 08:21:55|     4545|         45924|       NULL|     NULL|\n",
      "|    17|(51.2088,4.40834)|(51.2077,4.39846)|2019-09-22 08:27:38|2019-09-22 08:30:25|     3419|         25722|       2046|     1951|\n",
      "|    18|(51.2023,4.41208)|(51.2119,4.39894)|2019-09-22 08:41:48|2019-09-22 08:46:52|     1208|         31000|       1821|     2186|\n",
      "|    19|(51.1888,4.45039)|(51.2221,4.40467)|2019-09-22 08:50:08|2019-09-22 09:09:02|     5536|         59732|       6382|     2700|\n",
      "|    20|(51.2159,4.41073)|(51.2191,4.41596)|2019-09-22 08:29:42|2019-09-22 08:31:40|     6336|          NULL|       NULL|     NULL|\n",
      "+------+-----------------+-----------------+-------------------+-------------------+---------+--------------+-----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-23T09:26:23.759838Z",
     "start_time": "2025-04-23T09:25:53.411780Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ridesFactFromSource.printSchema()\n",
    "ridesFactFromSource.show(50)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ride_id: integer (nullable = true)\n",
      " |-- user_sk: string (nullable = true)\n",
      " |-- start_lock_id: integer (nullable = true)\n",
      " |-- end_lock_id: integer (nullable = true)\n",
      " |-- date_sk: long (nullable = true)\n",
      " |-- vehicle_id: integer (nullable = true)\n",
      " |-- ride_duration: double (nullable = true)\n",
      " |-- distance_km: double (nullable = true)\n",
      " |-- weather_id: integer (nullable = true)\n",
      " |-- md5: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 184:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-------------+-----------+-------+----------+-------------+-----------+----------+--------------------+\n",
      "|ride_id|             user_sk|start_lock_id|end_lock_id|date_sk|vehicle_id|ride_duration|distance_km|weather_id|                 md5|\n",
      "+-------+--------------------+-------------+-----------+-------+----------+-------------+-----------+----------+--------------------+\n",
      "|     13|d8f685cc-3412-4e0...|         5619|       2717|   2455|         2|          0.0|       5.38|         3|ec253130345dc6c41...|\n",
      "|     50|93e34254-7e69-47f...|         2962|       2973|   3916|         2|         5.23|       0.56|         3|e63cb075e0d01c00f...|\n",
      "|     67|f018bf1b-7ae3-484...|         1494|       5591|   3916|         1|        17.13|       4.46|         3|83caf42c81687b0cc...|\n",
      "|     79|b8f870fe-c89a-4fb...|         3260|       1411|   3916|         1|         13.2|       2.16|         3|3bc9fded18efd494f...|\n",
      "|    108|ea4639ae-6955-40b...|         6314|       4117|   3916|         1|        24.17|       4.64|         3|f1ade98fd45c945ae...|\n",
      "|    111|dfdc54bd-b3d7-46b...|         6570|       2739|   3916|         1|         4.38|       4.19|         3|18644bb43d048196e...|\n",
      "|    124|a7358511-c820-4af...|         3496|       4058|   3916|         1|         7.32|       1.19|         3|95c7d60f5877b308e...|\n",
      "|    135|8bca31e8-1922-4c8...|         2952|       1819|   3916|         1|        11.82|       3.19|         3|9e7c324a4bfe4a804...|\n",
      "|    136|f9ab1f92-794e-4a0...|         7542|        536|   3916|         2|         8.88|       0.77|         3|170bf711dc3010115...|\n",
      "|    206|145583f4-0ecf-42d...|         1445|       5118|   3916|         1|        15.65|       3.96|         3|d79127ae1a16bcaa2...|\n",
      "|    208|9703ba35-fec6-432...|          599|        918|   3916|         1|         8.92|       2.58|         3|6d9135a53dec71e7e...|\n",
      "|    218|eb7681cb-3287-48e...|         6549|       1147|   3916|         2|        11.05|       2.85|         3|d0ad83a1b57a7be97...|\n",
      "|    224|adb277e1-0598-457...|         1999|       3541|   3916|         1|         17.7|       3.99|         3|fb1260978e30b38e0...|\n",
      "|    232|6f564c62-d269-4ed...|          554|       2330|   3916|         1|         6.02|       1.89|         3|a4317a422ca2a91bc...|\n",
      "|    240|c90ccc2b-1b46-47e...|          186|       7267|   3916|         2|        20.97|       5.57|         3|068dec0d6887c6de5...|\n",
      "|      1|63bfe040-f1ff-4bb...|         4849|       3188|   2455|         1|          0.0|       3.49|         3|b8c91a2ea09aa5f7c...|\n",
      "|     18|0553493e-89b1-472...|         1821|       2186|   3916|         1|         5.07|       2.31|         3|2d7c426ea562b1f29...|\n",
      "|     35|d2a13cde-b1c1-481...|         1117|        677|   3916|         1|        10.47|       2.18|         3|785d876c7dc72f167...|\n",
      "|     36|cf9bac83-d8e9-4f8...|         6421|       1208|   3916|         1|        17.77|       5.04|         3|46fec4c82fe6518c8...|\n",
      "|     43|70dfccc4-39e2-41b...|         2507|       2365|   3916|         1|         6.52|       2.29|         3|44c550359cee153ac...|\n",
      "|     76|9031f92d-b6b2-448...|         4181|        979|   3916|         1|         4.82|       3.77|         3|6b5f7d612776f154a...|\n",
      "|    115|b17d099b-5ef7-4d2...|         6256|       4168|   3916|         1|         11.5|       8.04|         3|2ffec2cea6b38b215...|\n",
      "|    144|4a2b3924-7e60-48e...|         4799|       1044|   3916|         2|        16.98|       2.53|         3|9be5256a83e38df6f...|\n",
      "|    160|21581845-e936-463...|         2636|       3830|   3916|         1|        20.12|       2.78|         3|8444b42b778c0c3f7...|\n",
      "|    166|77f8ec56-4cd6-41e...|         2351|       2793|   3916|         1|         7.08|       1.24|         3|b13ac66fa65d86d6c...|\n",
      "|    170|d35fe247-74e7-4fc...|         3005|       2592|   3916|         2|         5.38|       3.35|         3|1658974d1ff2bad6e...|\n",
      "|    202|d6323fe5-63f9-42d...|         2821|       1190|   3916|         1|         8.22|       3.33|         3|0df2df3d7e381e08a...|\n",
      "|    204|f14d0560-32d2-48c...|         4482|       4279|   3916|         1|         1.95|       0.98|         3|129270404fa57ee6b...|\n",
      "|    230|8e13e97c-8658-498...|          723|       1473|   3916|         1|         7.12|       2.05|         3|9d75482a2ef068e30...|\n",
      "|    241|5b1ab9d2-2bdb-43d...|         2795|       4849|   3916|         1|        17.07|       3.47|         3|9a8ddcb876097288e...|\n",
      "|      7|a7551bdf-1268-463...|         1388|       3401|   2455|         1|          0.0|       7.07|         3|31690a4f5129c341b...|\n",
      "|     32|aa9ae7cb-5ee0-4a4...|         1056|        792|   3916|         1|          5.5|       2.23|         3|cfae1c133cb4c610d...|\n",
      "|     45|0c8a7a74-f6f2-4c3...|          615|       4172|   3916|         1|         4.48|       4.84|         3|7b55aee1fa7a9860f...|\n",
      "|     57|9a2bae01-ebf0-400...|          513|       2242|   3916|         1|        14.63|       2.47|         3|9f5de7d358200a1d6...|\n",
      "|     63|54970d35-83b6-4cd...|         2432|       2137|   3916|         1|         2.98|       2.32|         3|d5379b19bcec67d2b...|\n",
      "|    131|54dd2425-157e-43b...|         5099|       2466|   3916|         1|         8.18|       3.11|         3|3d9e6c90eba351995...|\n",
      "|    149|6820395d-0941-4b8...|         4076|       2430|   3916|         1|          4.5|       2.01|         3|7240a6e147e805f0b...|\n",
      "|    186|76f57250-8444-402...|         2788|       2626|   3916|         1|          7.3|       1.46|         3|ca6c4fdf4e891294e...|\n",
      "|    189|e3c0ff0b-e242-4c2...|         1421|       4066|   3916|         1|         10.0|       3.28|         3|55e6a82834b31514e...|\n",
      "|    191|c3e991ec-77eb-48a...|         3251|       2428|   3916|         1|          7.0|       1.75|         3|fd1e59406442168d4...|\n",
      "|     53|ca4e201b-7e1d-42f...|         4917|       1613|   3916|         1|        13.58|       2.59|         3|7cd4c31f30c0d3c5a...|\n",
      "|     70|31bf3b49-035f-438...|         3821|        626|   3916|         1|        18.87|       6.06|         3|7df3bdfd863dcbc3e...|\n",
      "|     80|ad9f11d3-cd7e-48a...|         3253|       5866|   3916|         1|        24.93|       6.75|         3|6a89533863c66103e...|\n",
      "|     99|5d4b48b3-9743-4f4...|         1865|       7530|   3916|         2|         3.08|       3.11|         3|95b61617b3c4cc12d...|\n",
      "|    105|b0b9c47c-0b8c-49f...|         1933|       1983|   3916|         2|          3.4|       0.97|         3|569638d01447f3df7...|\n",
      "|    116|c431774c-7281-482...|         5232|       2884|   3916|         2|        17.12|       5.43|         3|f42cd11d48d02a3f5...|\n",
      "|    127|859286c3-589d-456...|         1193|       1065|   3916|         2|         1.65|       3.09|         3|7d06817a50ffd1c7f...|\n",
      "|    140|36ca2050-338d-473...|         2116|        750|   3916|         1|         6.98|       2.05|         3|2309282e56656c797...|\n",
      "|    158|691e574f-767e-4c0...|         3241|       7111|   3916|         1|         28.0|       4.91|         3|1fb60eb50ead01307...|\n",
      "|    178|8bb908fd-38b0-47d...|         5755|       5653|   3916|         1|         4.88|       1.99|         3|c99c987ed23e51e37...|\n",
      "+-------+--------------------+-------------+-----------+-------+----------+-------------+-----------+----------+--------------------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Initial load\n",
    "The first time loading the fact table perform a FULL load. All data is written to the Delta Table.\n",
    "After initial load the code line has to be disabled"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "ridesFactFromSource.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"factRides\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-23T09:05:47.811624Z",
     "start_time": "2025-04-23T08:10:47.999474Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ridesFactFromSource' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[16]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[43mridesFactFromSource\u001B[49m.write.format(\u001B[33m\"\u001B[39m\u001B[33mdelta\u001B[39m\u001B[33m\"\u001B[39m).mode(\u001B[33m\"\u001B[39m\u001B[33moverwrite\u001B[39m\u001B[33m\"\u001B[39m).saveAsTable(\u001B[33m\"\u001B[39m\u001B[33mfactRides\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[31mNameError\u001B[39m: name 'ridesFactFromSource' is not defined"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Incremental load\n",
    "When previous runs where performend you can opt for a 'faster' incremental run that only writes away changes. UPDATES and INSERTS are performed in one run.\n",
    "In our solution we use an md5 based on all fields in the source table to detect changes. This is not the most efficient way to detect changes. A better way is to use a timestamp field in the source table and use that to detect changes. This is not implemented in this example."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------------+----------------+-----------------+\n",
      "|num_affected_rows|num_updated_rows|num_deleted_rows|num_inserted_rows|\n",
      "+-----------------+----------------+----------------+-----------------+\n",
      "|                0|               0|               0|                0|\n",
      "+-----------------+----------------+----------------+-----------------+\n"
     ]
    }
   ],
   "source": [
    "dt_factRides = DeltaTable.forPath(spark,\".\\spark-warehouse\\\\factrides\")\n",
    "dt_factRides.toDF().createOrReplaceTempView(\"factRides_current\")\n",
    "#Merge to perform updates (TODO: Implement md5 strategy)\n",
    "\n",
    "result = spark.sql(\"MERGE INTO factRides_current AS target \\\n",
    "      using factRides_new AS source ON target.rideID = source.rideID \\\n",
    "      WHEN MATCHED and source.MD5<>target.MD5 THEN UPDATE SET * \\\n",
    "      WHEN NOT MATCHED THEN INSERT *\")\n",
    "\n",
    "result.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-23T09:05:47.811792Z",
     "start_time": "2024-09-10T09:49:24.834540600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|     999|\n",
      "+--------+\n",
      "\n",
      "+-------+------+--------------------+--------+----------+--------------------+\n",
      "|OrderID|dateSK|          salesrepSK|count_mv|revenue_mv|                 md5|\n",
      "+-------+------+--------------------+--------+----------+--------------------+\n",
      "|      1|   650|b65df3d9-20dc-42d...|       1| 851804379|a237b06f2932af7dd...|\n",
      "+-------+------+--------------------+--------+----------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "# IMPORTANT: ALWAYS TEST THE CREATED CODE.\n",
    "# In this example I changed order 498 in the operational database and checked the change after the run.\n",
    "# spark.sql(\"select * from factsales f join dimsalesrep ds on f.salesrepSK = ds.salesrepSK where OrderID = 192  \").show()\n",
    "spark.sql(\"select count(*) from factrides\").show()\n",
    "spark.sql(\"select * from factrides where rideId=1\").show()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-23T09:05:47.811908Z",
     "start_time": "2024-09-10T09:49:36.262239400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Checking the history of your delta fact table"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fact' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# The history information is derived from the delta table log files. They contain a lot of information of all the actions performed on the table. In this case it tells us something about de merge operations. You can find statistics about the update and insert counts in the document.\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m \u001B[43mfact\u001B[49m\u001B[38;5;241m.\u001B[39mhistory()\u001B[38;5;241m.\u001B[39mshow(\u001B[38;5;241m10\u001B[39m,\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'fact' is not defined"
     ]
    }
   ],
   "source": [
    "# The history information is derived from the delta table log files. They contain a lot of information of all the actions performed on the table. In this case it tells us something about de merge operations. You can find statistics about the update and insert counts in the document.\n",
    "\n",
    "fact.history().show(10,False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-23T09:05:47.812019Z",
     "start_time": "2024-09-10T09:49:41.415008600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "spark.stop()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-23T09:05:47.812202Z",
     "start_time": "2025-03-19T11:01:55.677763Z"
    }
   },
   "outputs": [],
   "execution_count": 13
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
