{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Config stuff"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "import ConnectionConfig as cc\n",
    "from delta import DeltaTable\n",
    "cc.setupEnvironment()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-29T15:24:40.758429Z",
     "start_time": "2025-03-29T15:24:40.667161Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dynamically set JAVA_HOME: /Users/user/Library/Java/JavaVirtualMachines/temurin-21.0.2/Contents/Home\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "spark = cc.startLocalCluster(\"FACT_RIDE\")\n",
    "spark.getActiveSession()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-29T15:24:50.748772Z",
     "start_time": "2025-03-29T15:24:43.256844Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/user/Desktop/spark_and_hadop/spark-3.5.4-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/user/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/user/.ivy2/jars\n",
      "io.delta#delta-spark_2.12 added as a dependency\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      "org.postgresql#postgresql added as a dependency\n",
      "org.elasticsearch#elasticsearch-spark-30_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-b0d8ce68-69ab-4e5d-8b8b-6cb7aeeeb488;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.12;3.2.0 in central\n",
      "\tfound io.delta#delta-storage;3.2.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.4.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.4.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.3.2 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.9.1 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.6 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      "\tfound org.postgresql#postgresql;42.7.4 in central\n",
      "\tfound org.checkerframework#checker-qual;3.42.0 in central\n",
      "\tfound org.elasticsearch#elasticsearch-spark-30_2.12;8.15.2 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.12.19 in central\n",
      "\tfound javax.xml.bind#jaxb-api;2.3.1 in central\n",
      "\tfound org.apache.spark#spark-yarn_2.12;3.4.3 in central\n",
      ":: resolution report :: resolve 1018ms :: artifacts dl 36ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\tio.delta#delta-spark_2.12;3.2.0 from central in [default]\n",
      "\tio.delta#delta-storage;3.2.0 from central in [default]\n",
      "\tjavax.xml.bind#jaxb-api;2.3.1 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.3.2 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.4.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.4.0 from central in [default]\n",
      "\torg.apache.spark#spark-yarn_2.12;3.4.3 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.42.0 from central in [default]\n",
      "\torg.elasticsearch#elasticsearch-spark-30_2.12;8.15.2 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.postgresql#postgresql;42.7.4 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.19 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.6 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.9.1 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcommons-logging#commons-logging;1.1.1 by [commons-logging#commons-logging;1.1.3] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   21  |   0   |   0   |   1   ||   20  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-b0d8ce68-69ab-4e5d-8b8b-6cb7aeeeb488\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 20 already retrieved (0kB/26ms)\n",
      "25/03/29 16:24:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1077af690>"
      ],
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.151.48:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>FACT_RIDE</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Fact transformations\n",
    "This notebooks creates the sales fact table from scratch based on the operational source table \"sales\"\n",
    "When creating a fact table always follow the listed steps in order."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "#### 1 READ NECESSARY SOURCE TABLE(S) AND PERFORM TRANSFORMATIONS\n",
    "**When reading from the source table make sure you include all data necessary:**\n",
    "- to calculate the measure values\n",
    "- the source table keys that you have to use to lookup the correct surrogate keys in the dimension tables.\n",
    "\n",
    "**If more than one table is needed to gather the necesary information you can opt for one of two strategies:**\n",
    "- Use a select query when reading from the jdbc source with the spark.read operation. Avoid complex queries because the operational database needs a lot of resources to run those queries.\n",
    "- Perform a spark.read operation for each table separately and join the tables within Spark. The joins will take place on the cluster instead of the database. You limit the database recources used, but there can be a significant overhead of unnecessary data tranferred to the cluster.\n",
    "\n",
    "\n",
    "In this case we just rename Amount and create a default count_mv column.\n",
    "The transformations are minimal. In reality, transformations can be far more complex. If so, it can be advisable to work out the transforms in more then one step.*\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "cc.set_connectionProfile(\"default\")\n",
    "ride_src_df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", cc.create_jdbc()) \\\n",
    "    .option(\"driver\" , cc.get_Property(\"driver\")) \\\n",
    "    .option(\"dbtable\",\"rides\").option(\"user\", cc.get_Property(\"username\")) \\\n",
    "    .option(\"password\", cc.get_Property(\"password\")) \\\n",
    "    .option(\"partitionColumn\", \"rideid\") \\\n",
    "    .option(\"numPartitions\", 4) \\\n",
    "    .option(\"lowerBound\", 0) \\\n",
    "    .option(\"upperBound\", 1000) \\\n",
    "    .load()\n",
    "\n",
    "subscriptions_df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", cc.create_jdbc()) \\\n",
    "    .option(\"driver\" , cc.get_Property(\"driver\")) \\\n",
    "    .option(\"dbtable\",\"subscriptions\").option(\"user\", cc.get_Property(\"username\")) \\\n",
    "    .option(\"password\", cc.get_Property(\"password\")) \\\n",
    "    .option(\"partitionColumn\", \"subscriptionid\") \\\n",
    "    .option(\"numPartitions\", 4) \\\n",
    "    .option(\"lowerBound\", 0) \\\n",
    "    .option(\"upperBound\", 1000) \\\n",
    "    .load()\n",
    "\n",
    "# weather_df = spark.read.json(r'C:\\Users\\kkiva\\data4_project_group5\\examples\\weather\\*.json')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-29T15:43:34.050431Z",
     "start_time": "2025-03-29T15:43:33.861399Z"
    }
   },
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T15:24:59.010958Z",
     "start_time": "2025-03-29T15:24:56.910038Z"
    }
   },
   "cell_type": "code",
   "source": "subscriptions_df.show(20)",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.                 (0 + 1) / 1]\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/Desktop/data4_project_group5/myenv/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/Desktop/data4_project_group5/myenv/lib/python3.11/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "                                                                                \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[43msubscriptions_df\u001B[49m\u001B[43m.\u001B[49m\u001B[43mshow\u001B[49m\u001B[43m(\u001B[49m\u001B[32;43m20\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/data4_project_group5/myenv/lib/python3.11/site-packages/pyspark/sql/dataframe.py:947\u001B[39m, in \u001B[36mDataFrame.show\u001B[39m\u001B[34m(self, n, truncate, vertical)\u001B[39m\n\u001B[32m    887\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mshow\u001B[39m(\u001B[38;5;28mself\u001B[39m, n: \u001B[38;5;28mint\u001B[39m = \u001B[32m20\u001B[39m, truncate: Union[\u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mint\u001B[39m] = \u001B[38;5;28;01mTrue\u001B[39;00m, vertical: \u001B[38;5;28mbool\u001B[39m = \u001B[38;5;28;01mFalse\u001B[39;00m) -> \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    888\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Prints the first ``n`` rows to the console.\u001B[39;00m\n\u001B[32m    889\u001B[39m \n\u001B[32m    890\u001B[39m \u001B[33;03m    .. versionadded:: 1.3.0\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    945\u001B[39m \u001B[33;03m    name | Bob\u001B[39;00m\n\u001B[32m    946\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m947\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_show_string\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtruncate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvertical\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/data4_project_group5/myenv/lib/python3.11/site-packages/pyspark/sql/dataframe.py:965\u001B[39m, in \u001B[36mDataFrame._show_string\u001B[39m\u001B[34m(self, n, truncate, vertical)\u001B[39m\n\u001B[32m    959\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n\u001B[32m    960\u001B[39m         error_class=\u001B[33m\"\u001B[39m\u001B[33mNOT_BOOL\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    961\u001B[39m         message_parameters={\u001B[33m\"\u001B[39m\u001B[33marg_name\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mvertical\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33marg_type\u001B[39m\u001B[33m\"\u001B[39m: \u001B[38;5;28mtype\u001B[39m(vertical).\u001B[34m__name__\u001B[39m},\n\u001B[32m    962\u001B[39m     )\n\u001B[32m    964\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(truncate, \u001B[38;5;28mbool\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m truncate:\n\u001B[32m--> \u001B[39m\u001B[32m965\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_jdf\u001B[49m\u001B[43m.\u001B[49m\u001B[43mshowString\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m20\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvertical\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    966\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    967\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/data4_project_group5/myenv/lib/python3.11/site-packages/py4j/java_gateway.py:1321\u001B[39m, in \u001B[36mJavaMember.__call__\u001B[39m\u001B[34m(self, *args)\u001B[39m\n\u001B[32m   1314\u001B[39m args_command, temp_args = \u001B[38;5;28mself\u001B[39m._build_args(*args)\n\u001B[32m   1316\u001B[39m command = proto.CALL_COMMAND_NAME +\\\n\u001B[32m   1317\u001B[39m     \u001B[38;5;28mself\u001B[39m.command_header +\\\n\u001B[32m   1318\u001B[39m     args_command +\\\n\u001B[32m   1319\u001B[39m     proto.END_COMMAND_PART\n\u001B[32m-> \u001B[39m\u001B[32m1321\u001B[39m answer = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgateway_client\u001B[49m\u001B[43m.\u001B[49m\u001B[43msend_command\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcommand\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1322\u001B[39m return_value = get_return_value(\n\u001B[32m   1323\u001B[39m     answer, \u001B[38;5;28mself\u001B[39m.gateway_client, \u001B[38;5;28mself\u001B[39m.target_id, \u001B[38;5;28mself\u001B[39m.name)\n\u001B[32m   1325\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/data4_project_group5/myenv/lib/python3.11/site-packages/py4j/java_gateway.py:1038\u001B[39m, in \u001B[36mGatewayClient.send_command\u001B[39m\u001B[34m(self, command, retry, binary)\u001B[39m\n\u001B[32m   1036\u001B[39m connection = \u001B[38;5;28mself\u001B[39m._get_connection()\n\u001B[32m   1037\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1038\u001B[39m     response = \u001B[43mconnection\u001B[49m\u001B[43m.\u001B[49m\u001B[43msend_command\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcommand\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1039\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m binary:\n\u001B[32m   1040\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m response, \u001B[38;5;28mself\u001B[39m._create_connection_guard(connection)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/data4_project_group5/myenv/lib/python3.11/site-packages/py4j/clientserver.py:511\u001B[39m, in \u001B[36mClientServerConnection.send_command\u001B[39m\u001B[34m(self, command)\u001B[39m\n\u001B[32m    509\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    510\u001B[39m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m511\u001B[39m         answer = smart_decode(\u001B[38;5;28mself\u001B[39m.stream.readline()[:-\u001B[32m1\u001B[39m])\n\u001B[32m    512\u001B[39m         logger.debug(\u001B[33m\"\u001B[39m\u001B[33mAnswer received: \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[33m\"\u001B[39m.format(answer))\n\u001B[32m    513\u001B[39m         \u001B[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001B[39;00m\n\u001B[32m    514\u001B[39m         \u001B[38;5;66;03m# answer before the socket raises an error.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py:706\u001B[39m, in \u001B[36mSocketIO.readinto\u001B[39m\u001B[34m(self, b)\u001B[39m\n\u001B[32m    704\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[32m    705\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m706\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_sock\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrecv_into\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    707\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m timeout:\n\u001B[32m    708\u001B[39m         \u001B[38;5;28mself\u001B[39m._timeout_occurred = \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "#### 2 MAKE DIMENSION TABLES AVAILABLE AS VIEWS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "dim_date = spark.read.format(\"delta\").load(\"spark-warehouse/dimdate\")\n",
    "dim_vehicle = spark.read.format(\"delta\").load(\"spark-warehouse/dimvehicle\")\n",
    "dim_user = spark.read.format(\"delta\").load(\"spark-warehouse/dimuser\")\n",
    "dim_weather = spark.read.format(\"delta\").load(\"spark-warehouse/dimuser\")\n",
    "dim_lock = spark.read.format(\"delta\").load(\"spark-warehouse/dimlock\")\n",
    "\n",
    "dim_date.createOrReplaceTempView(\"dimDate\")\n",
    "dim_user.createOrReplaceTempView(\"dimUser\")\n",
    "dim_vehicle.createOrReplaceTempView(\"dimVehicle\")\n",
    "dim_weather.createOrReplaceTempView(\"dimWeather\")\n",
    "dim_lock.createOrReplaceTempView(\"dimLock\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-29T15:43:38.987372Z",
     "start_time": "2025-03-29T15:43:38.771931Z"
    }
   },
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T16:05:36.045903Z",
     "start_time": "2025-03-29T16:05:34.734827Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function to safely show table information\n",
    "def show_table_info(table_name, df):\n",
    "    print(f\"\\n=== {table_name} ===\")\n",
    "    print(f\"Row count: {df.count()}\")\n",
    "    print(\"Schema:\")\n",
    "    df.printSchema()\n",
    "    \n",
    "    # Show just a few rows to avoid memory issues\n",
    "    print(\"Sample data (5 rows):\")\n",
    "    df.limit(5).show(truncate=False)\n",
    "    \n",
    "    # Give Spark a moment between operations\n",
    "    import time\n",
    "    time.sleep(1)\n",
    "\n",
    "\n",
    "show_table_info(\"user sub\", subscriptions_df)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== user sub ===\n",
      "Row count: 78248\n",
      "Schema:\n",
      "root\n",
      " |-- subscriptionid: integer (nullable = true)\n",
      " |-- validfrom: date (nullable = true)\n",
      " |-- subscriptiontypeid: integer (nullable = true)\n",
      " |-- userid: integer (nullable = true)\n",
      "\n",
      "Sample data (5 rows):\n",
      "+--------------+----------+------------------+------+\n",
      "|subscriptionid|validfrom |subscriptiontypeid|userid|\n",
      "+--------------+----------+------------------+------+\n",
      "|1             |2019-08-02|3                 |1     |\n",
      "|2             |2019-11-12|1                 |1     |\n",
      "|3             |2020-12-14|1                 |1     |\n",
      "|4             |2021-10-05|2                 |2     |\n",
      "|5             |2022-09-17|3                 |3     |\n",
      "+--------------+----------+------------------+------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "#### 3 Build the fact table\n",
    "\n",
    "Within the creation of a fact table always perform these two tasks:\n",
    "1.   Include the measures of the fact\n",
    "2. Use the dimension tables to look up the surrogate keys that correspond with the natural key value. In case of SCD2 dimension use the scd_start en scd_end to find the correct version of the data in the dimension\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T15:49:52.865325Z",
     "start_time": "2025-03-29T15:49:52.811399Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "import math\n",
    "\n",
    "def haversine_km(lat1, lon1, lat2, lon2):\n",
    "    if None in (lat1, lon1, lat2, lon2):  # Handle NULL values safely\n",
    "        return None  \n",
    "    R = 6371  # Radius of Earth in km\n",
    "    lat1, lon1, lat2, lon2 = map(math.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = math.sin(dlat / 2) ** 2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2) ** 2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    return R * c\n",
    "\n",
    "# Register the UDF in Spark SQL\n",
    "haversine_udf = udf(haversine_km, DoubleType())\n",
    "spark.udf.register(\"haversine_km\", haversine_udf)\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/29 16:49:52 WARN SimpleFunctionRegistry: The function haversine_km replaced a previously registered function.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.udf.UserDefinedFunction at 0x10d337610>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "source": [
    "ride_src_df.createOrReplaceTempView(\"rides_source\")\n",
    "subscriptions_df.createOrReplaceTempView(\"subscriptions_source\")\n",
    "\n",
    "ridesFactFromSource = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        src.rideid AS ride_id, \n",
    "        du.userSK AS user_sk, \n",
    "        src.startlockid AS start_lock_id, \n",
    "        src.endlockid AS end_lock_id, \n",
    "        dd.date_sk AS date_sk, \n",
    "        dv.vehicleid AS vehicle_id, \n",
    "        (src.endtime - src.starttime) AS ride_duration, \n",
    "        CASE \n",
    "            WHEN dl_start.gpscoord IS NULL OR dl_end.gpscoord IS NULL THEN NULL \n",
    "            ELSE haversine_km(\n",
    "                CAST(SPLIT(REPLACE(dl_start.gpscoord, '(', ''), ',')[0] AS DOUBLE),\n",
    "                CAST(SPLIT(REPLACE(REPLACE(dl_start.gpscoord, ')', ''), '(', ''), ',')[1] AS DOUBLE),\n",
    "                CAST(SPLIT(REPLACE(dl_end.gpscoord, '(', ''), ',')[0] AS DOUBLE),\n",
    "                CAST(SPLIT(REPLACE(REPLACE(dl_end.gpscoord, ')', ''), '(', ''), ',')[1] AS DOUBLE)\n",
    "            )\n",
    "        END AS distance_km,\n",
    "        md5(concat(src.rideid, du.userSK, src.startlockid, src.endlockid, dd.date_sk, dv.vehicleid)) AS md5 \n",
    "    FROM rides_source AS src \n",
    "    LEFT OUTER JOIN subscriptions_source AS sub ON src.subscriptionid = sub.subscriptionid \n",
    "    LEFT OUTER JOIN dimUser AS du ON sub.userid = du.userSK \n",
    "    LEFT OUTER JOIN dimLock AS dl_start ON src.startlockid = dl_start.lockid\n",
    "    LEFT OUTER JOIN dimLock AS dl_end ON src.endlockid = dl_end.lockid\n",
    "    LEFT OUTER JOIN dimDate AS dd ON DATE(src.starttime) = dd.date \n",
    "    LEFT OUTER JOIN dimVehicle AS dv ON src.vehicleid = dv.vehicleid\n",
    "\"\"\")\n"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T16:03:22.332740Z",
     "start_time": "2025-03-29T16:03:22.047648Z"
    }
   },
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "ride_src_df.createOrReplaceTempView(\"rides_source\")\n",
    "subscriptions_df.createOrReplaceTempView(\"subscriptions_source\")"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T16:12:00.237604Z",
     "start_time": "2025-03-29T16:11:57.893069Z"
    }
   },
   "cell_type": "code",
   "source": [
    "xd = spark.sql(\"\"\"\n",
    "SELECT src.subscriptionid \n",
    "FROM rides_source AS src\n",
    "LEFT JOIN subscriptions_source AS sub ON sub.subscriptionid = src.subscriptionid\n",
    "WHERE sub.subscriptionid IS NULL;\n",
    "\n",
    "\"\"\")\n",
    "xd.show()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 204:==========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|subscriptionid|\n",
      "+--------------+\n",
      "|          NULL|\n",
      "|          NULL|\n",
      "|          NULL|\n",
      "|          NULL|\n",
      "|          NULL|\n",
      "|          NULL|\n",
      "|          NULL|\n",
      "|          NULL|\n",
      "|          NULL|\n",
      "|          NULL|\n",
      "|          NULL|\n",
      "|          NULL|\n",
      "|          NULL|\n",
      "|          NULL|\n",
      "|          NULL|\n",
      "|          NULL|\n",
      "|          NULL|\n",
      "|          NULL|\n",
      "|          NULL|\n",
      "|          NULL|\n",
      "+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-29T16:03:34.164185Z",
     "start_time": "2025-03-29T16:03:25.631390Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ridesFactFromSource.printSchema()\n",
    "ridesFactFromSource.show(20)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ride_id: integer (nullable = true)\n",
      " |-- user_sk: string (nullable = true)\n",
      " |-- start_lock_id: integer (nullable = true)\n",
      " |-- end_lock_id: integer (nullable = true)\n",
      " |-- date_sk: long (nullable = true)\n",
      " |-- vehicle_id: integer (nullable = true)\n",
      " |-- ride_duration: interval day to second (nullable = true)\n",
      " |-- distance_km: double (nullable = true)\n",
      " |-- md5: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 159:==========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------------+-----------+-------+----------+--------------------+------------------+----+\n",
      "|ride_id|user_sk|start_lock_id|end_lock_id|date_sk|vehicle_id|       ride_duration|       distance_km| md5|\n",
      "+-------+-------+-------------+-----------+-------+----------+--------------------+------------------+----+\n",
      "|      3|   NULL|         2046|       1951|   2455|      NULL|INTERVAL '-1095 0...|1.4949081239108692|NULL|\n",
      "|     11|   NULL|          985|       2148|   2455|      NULL|INTERVAL '-1095 0...| 2.067612187914813|NULL|\n",
      "|     13|   NULL|         5619|       2717|   2455|      NULL|INTERVAL '-1095 0...| 5.381957169382341|NULL|\n",
      "|     17|   NULL|         2046|       1951|   3916|      NULL|INTERVAL '0 00:02...|1.4949081239108692|NULL|\n",
      "|      1|   NULL|         4849|       3188|   2455|      NULL|INTERVAL '-1095 0...| 3.491018931475966|NULL|\n",
      "|      4|   NULL|         1821|       2186|   2455|      NULL|INTERVAL '-1095 0...|2.3114308809290756|NULL|\n",
      "|      5|   NULL|         6382|       2700|   2455|      NULL|INTERVAL '-1095 0...|   5.5271668137006|NULL|\n",
      "|      9|   NULL|           50|       2067|   2455|      NULL|INTERVAL '-1095 0...|1.2010324044122915|NULL|\n",
      "|     10|   NULL|         NULL|       NULL|   2455|      NULL|INTERVAL '-1095 0...|              NULL|NULL|\n",
      "|     14|   NULL|         3531|       3554|   2455|      NULL|INTERVAL '-1095 0...|               0.0|NULL|\n",
      "|     15|   NULL|         4849|       3188|   3916|      NULL|INTERVAL '0 00:14...| 3.491018931475966|NULL|\n",
      "|     18|   NULL|         1821|       2186|   3916|      NULL|INTERVAL '0 00:05...|2.3114308809290756|NULL|\n",
      "|     19|   NULL|         6382|       2700|   3916|      NULL|INTERVAL '0 00:18...|   5.5271668137006|NULL|\n",
      "|      2|   NULL|         NULL|       NULL|   2455|      NULL|INTERVAL '-1095 0...|              NULL|NULL|\n",
      "|      6|   NULL|         NULL|       NULL|   2455|      NULL|INTERVAL '-1095 0...|              NULL|NULL|\n",
      "|      7|   NULL|         1388|       3401|   2455|      NULL|INTERVAL '-1095 0...|  7.07224625834132|NULL|\n",
      "|      8|   NULL|         2572|         13|   2455|      NULL|INTERVAL '-1095 0...|1.1373196748870393|NULL|\n",
      "|     12|   NULL|         2039|       3038|   2455|      NULL|INTERVAL '-1095 0...| 3.050318586470677|NULL|\n",
      "|     16|   NULL|         NULL|       NULL|   3916|      NULL|INTERVAL '0 00:02...|              NULL|NULL|\n",
      "|     20|   NULL|         NULL|       NULL|   3916|      NULL|INTERVAL '0 00:01...|              NULL|NULL|\n",
      "+-------+-------+-------------+-----------+-------+----------+--------------------+------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Initial load\n",
    "The first time loading the fact table perform a FULL load. All data is written to the Delta Table.\n",
    "After initial load the code line has to be disabled"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "\n",
    "ridesFactFromSource.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"factRides\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-10T09:49:24.830124800Z",
     "start_time": "2024-09-10T09:49:10.188063700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Incremental load\n",
    "When previous runs where performend you can opt for a 'faster' incremental run that only writes away changes. UPDATES and INSERTS are performed in one run.\n",
    "In our solution we use an md5 based on all fields in the source table to detect changes. This is not the most efficient way to detect changes. A better way is to use a timestamp field in the source table and use that to detect changes. This is not implemented in this example."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------------+----------------+-----------------+\n",
      "|num_affected_rows|num_updated_rows|num_deleted_rows|num_inserted_rows|\n",
      "+-----------------+----------------+----------------+-----------------+\n",
      "|                0|               0|               0|                0|\n",
      "+-----------------+----------------+----------------+-----------------+\n"
     ]
    }
   ],
   "source": [
    "dt_factRides = DeltaTable.forPath(spark,\".\\spark-warehouse\\\\factrides\")\n",
    "dt_factRides.toDF().createOrReplaceTempView(\"factRides_current\")\n",
    "#Merge to perform updates (TODO: Implement md5 strategy)\n",
    "\n",
    "result = spark.sql(\"MERGE INTO factRides_current AS target \\\n",
    "      using factRides_new AS source ON target.rideID = source.rideID \\\n",
    "      WHEN MATCHED and source.MD5<>target.MD5 THEN UPDATE SET * \\\n",
    "      WHEN NOT MATCHED THEN INSERT *\")\n",
    "\n",
    "result.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-10T09:49:36.248336800Z",
     "start_time": "2024-09-10T09:49:24.834540600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|     999|\n",
      "+--------+\n",
      "\n",
      "+-------+------+--------------------+--------+----------+--------------------+\n",
      "|OrderID|dateSK|          salesrepSK|count_mv|revenue_mv|                 md5|\n",
      "+-------+------+--------------------+--------+----------+--------------------+\n",
      "|      1|   650|b65df3d9-20dc-42d...|       1| 851804379|a237b06f2932af7dd...|\n",
      "+-------+------+--------------------+--------+----------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "# IMPORTANT: ALWAYS TEST THE CREATED CODE.\n",
    "# In this example I changed order 498 in the operational database and checked the change after the run.\n",
    "# spark.sql(\"select * from factsales f join dimsalesrep ds on f.salesrepSK = ds.salesrepSK where OrderID = 192  \").show()\n",
    "spark.sql(\"select count(*) from factrides\").show()\n",
    "spark.sql(\"select * from factrides where rideId=1\").show()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-10T09:49:41.411568200Z",
     "start_time": "2024-09-10T09:49:36.262239400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Checking the history of your delta fact table"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fact' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# The history information is derived from the delta table log files. They contain a lot of information of all the actions performed on the table. In this case it tells us something about de merge operations. You can find statistics about the update and insert counts in the document.\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m \u001B[43mfact\u001B[49m\u001B[38;5;241m.\u001B[39mhistory()\u001B[38;5;241m.\u001B[39mshow(\u001B[38;5;241m10\u001B[39m,\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'fact' is not defined"
     ]
    }
   ],
   "source": [
    "# The history information is derived from the delta table log files. They contain a lot of information of all the actions performed on the table. In this case it tells us something about de merge operations. You can find statistics about the update and insert counts in the document.\n",
    "\n",
    "fact.history().show(10,False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-10T09:49:42.067046800Z",
     "start_time": "2024-09-10T09:49:41.415008600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "spark.stop()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-19T11:02:02.240620Z",
     "start_time": "2025-03-19T11:01:55.677763Z"
    }
   },
   "outputs": [],
   "execution_count": 13
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
