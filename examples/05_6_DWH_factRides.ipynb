{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Config stuff"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "import ConnectionConfig as cc\n",
    "from delta import DeltaTable\n",
    "cc.setupEnvironment()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-29T16:24:13.192473Z",
     "start_time": "2025-03-29T16:24:13.092214Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dynamically set JAVA_HOME: /Users/user/Library/Java/JavaVirtualMachines/temurin-21.0.2/Contents/Home\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "spark = cc.startLocalCluster(\"FACT_RIDE\")\n",
    "spark.getActiveSession()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-29T16:41:37.760328Z",
     "start_time": "2025-03-29T16:41:37.512050Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1050af210>"
      ],
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.151.48:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>FACT_RIDE</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Fact transformations\n",
    "This notebooks creates the sales fact table from scratch based on the operational source table \"sales\"\n",
    "When creating a fact table always follow the listed steps in order."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "#### 1 READ NECESSARY SOURCE TABLE(S) AND PERFORM TRANSFORMATIONS\n",
    "**When reading from the source table make sure you include all data necessary:**\n",
    "- to calculate the measure values\n",
    "- the source table keys that you have to use to lookup the correct surrogate keys in the dimension tables.\n",
    "\n",
    "**If more than one table is needed to gather the necesary information you can opt for one of two strategies:**\n",
    "- Use a select query when reading from the jdbc source with the spark.read operation. Avoid complex queries because the operational database needs a lot of resources to run those queries.\n",
    "- Perform a spark.read operation for each table separately and join the tables within Spark. The joins will take place on the cluster instead of the database. You limit the database recources used, but there can be a significant overhead of unnecessary data tranferred to the cluster.\n",
    "\n",
    "\n",
    "In this case we just rename Amount and create a default count_mv column.\n",
    "The transformations are minimal. In reality, transformations can be far more complex. If so, it can be advisable to work out the transforms in more then one step.*\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "cc.set_connectionProfile(\"default\")\n",
    "ride_src_df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", cc.create_jdbc()) \\\n",
    "    .option(\"driver\" , cc.get_Property(\"driver\")) \\\n",
    "    .option(\"dbtable\",\"rides\").option(\"user\", cc.get_Property(\"username\")) \\\n",
    "    .option(\"password\", cc.get_Property(\"password\")) \\\n",
    "    .option(\"partitionColumn\", \"rideid\") \\\n",
    "    .option(\"numPartitions\", 4) \\\n",
    "    .option(\"lowerBound\", 0) \\\n",
    "    .option(\"upperBound\", 1000) \\\n",
    "    .load()\n",
    "\n",
    "subscriptions_df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", cc.create_jdbc()) \\\n",
    "    .option(\"driver\" , cc.get_Property(\"driver\")) \\\n",
    "    .option(\"dbtable\",\"subscriptions\").option(\"user\", cc.get_Property(\"username\")) \\\n",
    "    .option(\"password\", cc.get_Property(\"password\")) \\\n",
    "    .option(\"partitionColumn\", \"subscriptionid\") \\\n",
    "    .option(\"numPartitions\", 4) \\\n",
    "    .option(\"lowerBound\", 0) \\\n",
    "    .option(\"upperBound\", 1000) \\\n",
    "    .load()\n",
    "\n",
    "\n",
    "# weather_df = spark.read.json(r'C:\\Users\\kkiva\\data4_project_group5\\examples\\weather\\*.json')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-29T16:41:40.115102Z",
     "start_time": "2025-03-29T16:41:39.963548Z"
    }
   },
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T16:41:42.019582Z",
     "start_time": "2025-03-29T16:41:42.012588Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, IntegerType, ArrayType\n",
    "\n",
    "# Define the schema for the JSON structure based on the provided response\n",
    "weather_schema = StructType([\n",
    "    StructField(\"zipCode\", StringType(), True),\n",
    "    StructField(\"coord\", StructType([\n",
    "        StructField(\"lon\", FloatType(), True),\n",
    "        StructField(\"lat\", FloatType(), True)\n",
    "    ]), True),\n",
    "    StructField(\"weather\", ArrayType(StructType([\n",
    "        StructField(\"id\", IntegerType(), True),\n",
    "        StructField(\"main\", StringType(), True),\n",
    "        StructField(\"description\", StringType(), True),\n",
    "        StructField(\"icon\", StringType(), True)\n",
    "    ])), True),\n",
    "    StructField(\"base\", StringType(), True),\n",
    "    StructField(\"main\", StructType([\n",
    "        StructField(\"temp\", FloatType(), True),\n",
    "        StructField(\"feels_like\", FloatType(), True),\n",
    "        StructField(\"temp_min\", FloatType(), True),\n",
    "        StructField(\"temp_max\", FloatType(), True),\n",
    "        StructField(\"pressure\", IntegerType(), True),\n",
    "        StructField(\"humidity\", IntegerType(), True),\n",
    "        StructField(\"sea_level\", IntegerType(), True),\n",
    "        StructField(\"grnd_level\", IntegerType(), True)\n",
    "    ]), True),\n",
    "    StructField(\"visibility\", IntegerType(), True),\n",
    "    StructField(\"wind\", StructType([\n",
    "        StructField(\"speed\", FloatType(), True),\n",
    "        StructField(\"deg\", IntegerType(), True),\n",
    "        StructField(\"gust\", FloatType(), True)\n",
    "    ]), True),\n",
    "    StructField(\"rain\", StructType([\n",
    "        StructField(\"1h\", FloatType(), True)\n",
    "    ]), True),\n",
    "    StructField(\"clouds\", StructType([\n",
    "        StructField(\"all\", IntegerType(), True)\n",
    "    ]), True),\n",
    "    StructField(\"dt\", IntegerType(), True),\n",
    "    StructField(\"sys\", StructType([\n",
    "        StructField(\"type\", IntegerType(), True),\n",
    "        StructField(\"id\", IntegerType(), True),\n",
    "        StructField(\"country\", StringType(), True),\n",
    "        StructField(\"sunrise\", IntegerType(), True),\n",
    "        StructField(\"sunset\", IntegerType(), True)\n",
    "    ]), True),\n",
    "    StructField(\"timezone\", IntegerType(), True),\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"cod\", IntegerType(), True)\n",
    "])\n"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T16:34:33.811660Z",
     "start_time": "2025-03-29T16:34:33.790039Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'StructType' object has no attribute 'createOrReplaceTempView'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mAttributeError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[15]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[43mweather_schema\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcreateOrReplaceTempView\u001B[49m(\u001B[33m\"\u001B[39m\u001B[33mweatherdimx\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[31mAttributeError\u001B[39m: 'StructType' object has no attribute 'createOrReplaceTempView'"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T16:41:45.073920Z",
     "start_time": "2025-03-29T16:41:44.636140Z"
    }
   },
   "cell_type": "code",
   "source": [
    "weather_df = spark.read.option(\"multiline\", \"true\").schema(weather_schema).json(\"weather/*.json\")\n",
    "weather_df.printSchema()\n",
    "weather_df.show(5, truncate=False)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- zipCode: string (nullable = true)\n",
      " |-- coord: struct (nullable = true)\n",
      " |    |-- lon: float (nullable = true)\n",
      " |    |-- lat: float (nullable = true)\n",
      " |-- weather: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- id: integer (nullable = true)\n",
      " |    |    |-- main: string (nullable = true)\n",
      " |    |    |-- description: string (nullable = true)\n",
      " |    |    |-- icon: string (nullable = true)\n",
      " |-- base: string (nullable = true)\n",
      " |-- main: struct (nullable = true)\n",
      " |    |-- temp: float (nullable = true)\n",
      " |    |-- feels_like: float (nullable = true)\n",
      " |    |-- temp_min: float (nullable = true)\n",
      " |    |-- temp_max: float (nullable = true)\n",
      " |    |-- pressure: integer (nullable = true)\n",
      " |    |-- humidity: integer (nullable = true)\n",
      " |    |-- sea_level: integer (nullable = true)\n",
      " |    |-- grnd_level: integer (nullable = true)\n",
      " |-- visibility: integer (nullable = true)\n",
      " |-- wind: struct (nullable = true)\n",
      " |    |-- speed: float (nullable = true)\n",
      " |    |-- deg: integer (nullable = true)\n",
      " |    |-- gust: float (nullable = true)\n",
      " |-- rain: struct (nullable = true)\n",
      " |    |-- 1h: float (nullable = true)\n",
      " |-- clouds: struct (nullable = true)\n",
      " |    |-- all: integer (nullable = true)\n",
      " |-- dt: integer (nullable = true)\n",
      " |-- sys: struct (nullable = true)\n",
      " |    |-- type: integer (nullable = true)\n",
      " |    |-- id: integer (nullable = true)\n",
      " |    |-- country: string (nullable = true)\n",
      " |    |-- sunrise: integer (nullable = true)\n",
      " |    |-- sunset: integer (nullable = true)\n",
      " |-- timezone: integer (nullable = true)\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- cod: integer (nullable = true)\n",
      "\n",
      "+-------+------------------+--------------------------------------+--------+---------------------------------------------+----------+-----------------+-----+------+----------+----------------------------------------+--------+------+-----+---+\n",
      "|zipCode|coord             |weather                               |base    |main                                         |visibility|wind             |rain |clouds|dt        |sys                                     |timezone|id    |name |cod|\n",
      "+-------+------------------+--------------------------------------+--------+---------------------------------------------+----------+-----------------+-----+------+----------+----------------------------------------+--------+------+-----+---+\n",
      "|2018   |{4.42545, 51.2037}|[{0, Pleasant, pleasant weather, 01d}]|stations|{11.6, 11.6, 10.6, 13.1, 1015, 50, 1015, 933}|10000     |{14.0, 306, 14.5}|{0.0}|{0}   |1742295600|{2, 2075663, BE, 1742284800, 1742328000}|3600    |201800|Zocca|200|\n",
      "|2018   |{4.42545, 51.2037}|[{0, Pleasant, pleasant weather, 01d}]|stations|{11.6, 11.6, 10.6, 13.1, 1015, 50, 1015, 933}|10000     |{14.0, 306, 14.5}|{0.0}|{0}   |1742299200|{2, 2075663, BE, 1742288400, 1742331600}|3600    |201800|Zocca|200|\n",
      "|2018   |{4.42545, 51.2037}|[{0, Pleasant, pleasant weather, 01d}]|stations|{11.6, 11.6, 10.6, 13.1, 1015, 50, 1015, 933}|10000     |{14.0, 306, 14.5}|{0.0}|{0}   |1742302800|{2, 2075663, BE, 1742292000, 1742335200}|3600    |201800|Zocca|200|\n",
      "|2020   |{4.39731, 51.1894}|[{0, Pleasant, pleasant weather, 01d}]|stations|{11.5, 11.5, 10.5, 13.0, 1015, 50, 1015, 933}|10000     |{14.0, 308, 14.5}|{0.0}|{0}   |1742295600|{2, 2075663, BE, 1742284800, 1742328000}|3600    |202000|Zocca|200|\n",
      "|2020   |{4.39731, 51.1894}|[{0, Pleasant, pleasant weather, 01d}]|stations|{11.5, 11.5, 10.5, 13.0, 1015, 50, 1015, 933}|10000     |{14.0, 308, 14.5}|{0.0}|{0}   |1742299200|{2, 2075663, BE, 1742288400, 1742331600}|3600    |202000|Zocca|200|\n",
      "+-------+------------------+--------------------------------------+--------+---------------------------------------------+----------+-----------------+-----+------+----------+----------------------------------------+--------+------+-----+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "#### 2 MAKE DIMENSION TABLES AVAILABLE AS VIEWS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "dim_date = spark.read.format(\"delta\").load(\"spark-warehouse/dimdate\")\n",
    "dim_vehicle = spark.read.format(\"delta\").load(\"spark-warehouse/dimvehicle\")\n",
    "dim_user = spark.read.format(\"delta\").load(\"spark-warehouse/dimuser\")\n",
    "dim_weather = spark.read.format(\"delta\").load(\"spark-warehouse/dimweather\")\n",
    "dim_lock = spark.read.format(\"delta\").load(\"spark-warehouse/dimlock\")\n",
    "\n",
    "dim_date.createOrReplaceTempView(\"dimDate\")\n",
    "dim_user.createOrReplaceTempView(\"dimUser\")\n",
    "dim_vehicle.createOrReplaceTempView(\"dimVehicle\")\n",
    "dim_weather.createOrReplaceTempView(\"dimWeather\")\n",
    "dim_lock.createOrReplaceTempView(\"dimLock\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-29T16:42:22.189379Z",
     "start_time": "2025-03-29T16:42:21.853940Z"
    }
   },
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "#### 3 Build the fact table\n",
    "\n",
    "Within the creation of a fact table always perform these two tasks:\n",
    "1.   Include the measures of the fact\n",
    "2. Use the dimension tables to look up the surrogate keys that correspond with the natural key value. In case of SCD2 dimension use the scd_start en scd_end to find the correct version of the data in the dimension\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T16:42:25.055982Z",
     "start_time": "2025-03-29T16:42:24.975679Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "import math\n",
    "\n",
    "def haversine_km(lat1, lon1, lat2, lon2):\n",
    "    if None in (lat1, lon1, lat2, lon2):  # Handle NULL values safely\n",
    "        return None  \n",
    "    R = 6371  # Radius of Earth in km\n",
    "    lat1, lon1, lat2, lon2 = map(math.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = math.sin(dlat / 2) ** 2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2) ** 2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    return R * c\n",
    "\n",
    "# Register the UDF in Spark SQL\n",
    "haversine_udf = udf(haversine_km, DoubleType())\n",
    "spark.udf.register(\"haversine_km\", haversine_udf)\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.udf.UserDefinedFunction at 0x103a10ed0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "source": [
    "ride_src_df.createOrReplaceTempView(\"rides_source\")\n",
    "subscriptions_df.createOrReplaceTempView(\"subscriptions_source\")\n",
    "\n",
    "# Create temp views for the required dataframes\n",
    "weather_df.createOrReplaceTempView(\"weather_data\")\n",
    "\n",
    "ridesFactFromSource = spark.sql(\"\"\"\n",
    "    SELECT src.rideid AS ride_id, \n",
    "           du.userSK AS user_sk, \n",
    "           src.startlockid AS start_lock_id, \n",
    "           src.endlockid AS end_lock_id, \n",
    "           dd.date_sk AS date_sk, \n",
    "           dv.vehicleid AS vehicle_id, \n",
    "           (src.endtime - src.starttime) AS ride_duration, \n",
    "           haversine_km(\n",
    "               CAST(SPLIT(REPLACE(dl_start.gpscoord, '(', ''), ',')[0] AS DOUBLE),\n",
    "               CAST(SPLIT(REPLACE(REPLACE(dl_start.gpscoord, ')', ''), '(', ''), ',')[1] AS DOUBLE),\n",
    "               CAST(SPLIT(REPLACE(dl_end.gpscoord, '(', ''), ',')[0] AS DOUBLE),\n",
    "               CAST(SPLIT(REPLACE(REPLACE(dl_end.gpscoord, ')', ''), '(', ''), ',')[1] AS DOUBLE)\n",
    "           ) AS distance_km, dw.weather_id,  \n",
    "           md5(concat(src.rideid, du.userSK, src.startlockid, src.endlockid, dd.date_sk, dv.vehicleid)) AS md5 \n",
    "    FROM rides_source AS src \n",
    "    LEFT OUTER JOIN subscriptions_source AS sub ON src.subscriptionid = sub.subscriptionid \n",
    "    LEFT OUTER JOIN dimUser AS du ON sub.userid = du.userSK \n",
    "    LEFT OUTER JOIN dimLock AS dl_start ON src.startlockid = dl_start.lockid  \n",
    "    LEFT OUTER JOIN dimLock AS dl_end ON src.endlockid = dl_end.lockid  \n",
    "    LEFT OUTER JOIN dimDate AS dd ON DATE(src.starttime) = dd.date \n",
    "    LEFT OUTER JOIN dimVehicle AS dv ON src.vehicleid = dv.vehicleid \n",
    "    LEFT OUTER JOIN weather_data AS wd \n",
    "        ON dl_start.zipcode = wd.zipcode  -- Match postal code (from dimLock) with weather data\n",
    "        AND date_trunc('hour', src.starttime) = date_trunc('hour', wd.weather_time)  -- Match the rounded hour of the trip's start time with weather data time\n",
    "    LEFT OUTER JOIN dimWeather AS dw \n",
    "        ON wd.weather_condition = dw.weather_condition  -- Match weather condition from JSON to weather dimension\n",
    "\"\"\")\n"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T16:47:59.734875Z",
     "start_time": "2025-03-29T16:47:59.418412Z"
    }
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `wd`.`weather_time` cannot be resolved. Did you mean one of the following? [`wd`.`weather`, `dd`.`date_sk`, `src`.`endtime`, `du`.`userid`, `wd`.`base`].; line 25 pos 67;\n'Project ['src.rideid AS ride_id#2867, 'du.userSK AS user_sk#2868, 'src.startlockid AS start_lock_id#2869, 'src.endlockid AS end_lock_id#2870, 'dd.date_sk AS date_sk#2871, 'dv.vehicleid AS vehicle_id#2872, ('src.endtime - 'src.starttime) AS ride_duration#2873, 'haversine_km(cast('SPLIT('REPLACE('dl_start.gpscoord, (, ), ,)[0] as double), cast('SPLIT('REPLACE('REPLACE('dl_start.gpscoord, ), ), (, ), ,)[1] as double), cast('SPLIT('REPLACE('dl_end.gpscoord, (, ), ,)[0] as double), cast('SPLIT('REPLACE('REPLACE('dl_end.gpscoord, ), ), (, ), ,)[1] as double)) AS distance_km#2874, 'dw.weather_id, 'md5('concat('src.rideid, 'du.userSK, 'src.startlockid, 'src.endlockid, 'dd.date_sk, 'dv.vehicleid)) AS md5#2875]\n+- 'Join LeftOuter, ('wd.weather_condition = 'dw.weather_condition)\n   :- 'Join LeftOuter, ((zipcode#1289 = zipcode#863) AND (date_trunc(hour, starttime#840, Some(Europe/Brussels)) = 'date_trunc(hour, 'wd.weather_time)))\n   :  :- Join LeftOuter, (vehicleid#842 = vehicleid#1218)\n   :  :  :- Join LeftOuter, (cast(starttime#840 as date) = date#1201)\n   :  :  :  :- Join LeftOuter, (endlockid#845 = lockid#2876)\n   :  :  :  :  :- Join LeftOuter, (startlockid#844 = lockid#1284)\n   :  :  :  :  :  :- Join LeftOuter, (userid#858 = cast(userSK#1224 as int))\n   :  :  :  :  :  :  :- Join LeftOuter, (subscriptionid#843 = subscriptionid#855)\n   :  :  :  :  :  :  :  :- SubqueryAlias src\n   :  :  :  :  :  :  :  :  +- SubqueryAlias rides_source\n   :  :  :  :  :  :  :  :     +- View (`rides_source`, [rideid#837,startpoint#838,endpoint#839,starttime#840,endtime#841,vehicleid#842,subscriptionid#843,startlockid#844,endlockid#845])\n   :  :  :  :  :  :  :  :        +- Relation [rideid#837,startpoint#838,endpoint#839,starttime#840,endtime#841,vehicleid#842,subscriptionid#843,startlockid#844,endlockid#845] JDBCRelation(rides) [numPartitions=4]\n   :  :  :  :  :  :  :  +- SubqueryAlias sub\n   :  :  :  :  :  :  :     +- SubqueryAlias subscriptions_source\n   :  :  :  :  :  :  :        +- View (`subscriptions_source`, [subscriptionid#855,validfrom#856,subscriptiontypeid#857,userid#858])\n   :  :  :  :  :  :  :           +- Relation [subscriptionid#855,validfrom#856,subscriptiontypeid#857,userid#858] JDBCRelation(subscriptions) [numPartitions=4]\n   :  :  :  :  :  :  +- SubqueryAlias du\n   :  :  :  :  :  :     +- SubqueryAlias dimuser\n   :  :  :  :  :  :        +- View (`dimUser`, [userSK#1224,userid#1225,name#1226,email#1227,street#1228,number#1229,zipcode#1230,city#1231,country_code#1232,scd_start#1233,scd_end#1234,md5#1235,current#1236])\n   :  :  :  :  :  :           +- Relation [userSK#1224,userid#1225,name#1226,email#1227,street#1228,number#1229,zipcode#1230,city#1231,country_code#1232,scd_start#1233,scd_end#1234,md5#1235,current#1236] parquet\n   :  :  :  :  :  +- SubqueryAlias dl_start\n   :  :  :  :  :     +- SubqueryAlias dimlock\n   :  :  :  :  :        +- View (`dimLock`, [lockid#1284,stationid#1285,stationnr#1286,street#1287,number#1288,zipcode#1289,district#1290,gpscoord#1291])\n   :  :  :  :  :           +- Relation [lockid#1284,stationid#1285,stationnr#1286,street#1287,number#1288,zipcode#1289,district#1290,gpscoord#1291] parquet\n   :  :  :  :  +- SubqueryAlias dl_end\n   :  :  :  :     +- SubqueryAlias dimlock\n   :  :  :  :        +- View (`dimLock`, [lockid#2876,stationid#2877,stationnr#2878,street#2879,number#2880,zipcode#2881,district#2882,gpscoord#2883])\n   :  :  :  :           +- Relation [lockid#2876,stationid#2877,stationnr#2878,street#2879,number#2880,zipcode#2881,district#2882,gpscoord#2883] parquet\n   :  :  :  +- SubqueryAlias dd\n   :  :  :     +- SubqueryAlias dimdate\n   :  :  :        +- View (`dimDate`, [date_sk#1200L,date#1201,year#1202,quarter#1203,month_nr#1204,month_name#1205,day_nr#1206,day_name#1207,is_weekday#1208])\n   :  :  :           +- Relation [date_sk#1200L,date#1201,year#1202,quarter#1203,month_nr#1204,month_name#1205,day_nr#1206,day_name#1207,is_weekday#1208] parquet\n   :  :  +- SubqueryAlias dv\n   :  :     +- SubqueryAlias dimvehicle\n   :  :        +- View (`dimVehicle`, [vehicleid#1218,biketypeid#1219,biketypedescription#1220])\n   :  :           +- Relation [vehicleid#1218,biketypeid#1219,biketypedescription#1220] parquet\n   :  +- SubqueryAlias wd\n   :     +- SubqueryAlias weather_data\n   :        +- View (`weather_data`, [zipCode#863,coord#864,weather#865,base#866,main#867,visibility#868,wind#869,rain#870,clouds#871,dt#872,sys#873,timezone#874,id#875,name#876,cod#877])\n   :           +- Relation [zipCode#863,coord#864,weather#865,base#866,main#867,visibility#868,wind#869,rain#870,clouds#871,dt#872,sys#873,timezone#874,id#875,name#876,cod#877] json\n   +- SubqueryAlias dw\n      +- SubqueryAlias dimweather\n         +- View (`dimWeather`, [weather_id#1280,weather_condition#1281])\n            +- Relation [weather_id#1280,weather_condition#1281] parquet\n",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mAnalysisException\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[31]\u001B[39m\u001B[32m, line 7\u001B[39m\n\u001B[32m      4\u001B[39m \u001B[38;5;66;03m# Create temp views for the required dataframes\u001B[39;00m\n\u001B[32m      5\u001B[39m weather_df.createOrReplaceTempView(\u001B[33m\"\u001B[39m\u001B[33mweather_data\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m----> \u001B[39m\u001B[32m7\u001B[39m ridesFactFromSource = \u001B[43mspark\u001B[49m\u001B[43m.\u001B[49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\"\"\u001B[39;49m\n\u001B[32m      8\u001B[39m \u001B[33;43m    SELECT src.rideid AS ride_id, \u001B[39;49m\n\u001B[32m      9\u001B[39m \u001B[33;43m           du.userSK AS user_sk, \u001B[39;49m\n\u001B[32m     10\u001B[39m \u001B[33;43m           src.startlockid AS start_lock_id, \u001B[39;49m\n\u001B[32m     11\u001B[39m \u001B[33;43m           src.endlockid AS end_lock_id, \u001B[39;49m\n\u001B[32m     12\u001B[39m \u001B[33;43m           dd.date_sk AS date_sk, \u001B[39;49m\n\u001B[32m     13\u001B[39m \u001B[33;43m           dv.vehicleid AS vehicle_id, \u001B[39;49m\n\u001B[32m     14\u001B[39m \u001B[33;43m           (src.endtime - src.starttime) AS ride_duration, \u001B[39;49m\n\u001B[32m     15\u001B[39m \u001B[33;43m           haversine_km(\u001B[39;49m\n\u001B[32m     16\u001B[39m \u001B[33;43m               CAST(SPLIT(REPLACE(dl_start.gpscoord, \u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m(\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m, \u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m), \u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m,\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m)[0] AS DOUBLE),\u001B[39;49m\n\u001B[32m     17\u001B[39m \u001B[33;43m               CAST(SPLIT(REPLACE(REPLACE(dl_start.gpscoord, \u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m)\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m, \u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m), \u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m(\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m, \u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m), \u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m,\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m)[1] AS DOUBLE),\u001B[39;49m\n\u001B[32m     18\u001B[39m \u001B[33;43m               CAST(SPLIT(REPLACE(dl_end.gpscoord, \u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m(\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m, \u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m), \u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m,\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m)[0] AS DOUBLE),\u001B[39;49m\n\u001B[32m     19\u001B[39m \u001B[33;43m               CAST(SPLIT(REPLACE(REPLACE(dl_end.gpscoord, \u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m)\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m, \u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m), \u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m(\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m, \u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m), \u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m,\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m)[1] AS DOUBLE)\u001B[39;49m\n\u001B[32m     20\u001B[39m \u001B[33;43m           ) AS distance_km, dw.weather_id,  \u001B[39;49m\n\u001B[32m     21\u001B[39m \u001B[33;43m           md5(concat(src.rideid, du.userSK, src.startlockid, src.endlockid, dd.date_sk, dv.vehicleid)) AS md5 \u001B[39;49m\n\u001B[32m     22\u001B[39m \u001B[33;43m    FROM rides_source AS src \u001B[39;49m\n\u001B[32m     23\u001B[39m \u001B[33;43m    LEFT OUTER JOIN subscriptions_source AS sub ON src.subscriptionid = sub.subscriptionid \u001B[39;49m\n\u001B[32m     24\u001B[39m \u001B[33;43m    LEFT OUTER JOIN dimUser AS du ON sub.userid = du.userSK \u001B[39;49m\n\u001B[32m     25\u001B[39m \u001B[33;43m    LEFT OUTER JOIN dimLock AS dl_start ON src.startlockid = dl_start.lockid  \u001B[39;49m\n\u001B[32m     26\u001B[39m \u001B[33;43m    LEFT OUTER JOIN dimLock AS dl_end ON src.endlockid = dl_end.lockid  \u001B[39;49m\n\u001B[32m     27\u001B[39m \u001B[33;43m    LEFT OUTER JOIN dimDate AS dd ON DATE(src.starttime) = dd.date \u001B[39;49m\n\u001B[32m     28\u001B[39m \u001B[33;43m    LEFT OUTER JOIN dimVehicle AS dv ON src.vehicleid = dv.vehicleid \u001B[39;49m\n\u001B[32m     29\u001B[39m \u001B[33;43m    LEFT OUTER JOIN weather_data AS wd \u001B[39;49m\n\u001B[32m     30\u001B[39m \u001B[33;43m        ON dl_start.zipcode = wd.zipcode  -- Match postal code (from dimLock) with weather data\u001B[39;49m\n\u001B[32m     31\u001B[39m \u001B[33;43m        AND date_trunc(\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mhour\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m, src.starttime) = date_trunc(\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mhour\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m, wd.weather_time)  -- Match the rounded hour of the trip\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[33;43ms start time with weather data time\u001B[39;49m\n\u001B[32m     32\u001B[39m \u001B[33;43m    LEFT OUTER JOIN dimWeather AS dw \u001B[39;49m\n\u001B[32m     33\u001B[39m \u001B[33;43m        ON wd.weather_condition = dw.weather_condition  -- Match weather condition from JSON to weather dimension\u001B[39;49m\n\u001B[32m     34\u001B[39m \u001B[33;43m\"\"\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/data4_project_group5/myenv/lib/python3.11/site-packages/pyspark/sql/session.py:1631\u001B[39m, in \u001B[36mSparkSession.sql\u001B[39m\u001B[34m(self, sqlQuery, args, **kwargs)\u001B[39m\n\u001B[32m   1627\u001B[39m         \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m._jvm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1628\u001B[39m         litArgs = \u001B[38;5;28mself\u001B[39m._jvm.PythonUtils.toArray(\n\u001B[32m   1629\u001B[39m             [_to_java_column(lit(v)) \u001B[38;5;28;01mfor\u001B[39;00m v \u001B[38;5;129;01min\u001B[39;00m (args \u001B[38;5;129;01mor\u001B[39;00m [])]\n\u001B[32m   1630\u001B[39m         )\n\u001B[32m-> \u001B[39m\u001B[32m1631\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_jsparkSession\u001B[49m\u001B[43m.\u001B[49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43msqlQuery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlitArgs\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m)\n\u001B[32m   1632\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m   1633\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(kwargs) > \u001B[32m0\u001B[39m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/data4_project_group5/myenv/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001B[39m, in \u001B[36mJavaMember.__call__\u001B[39m\u001B[34m(self, *args)\u001B[39m\n\u001B[32m   1316\u001B[39m command = proto.CALL_COMMAND_NAME +\\\n\u001B[32m   1317\u001B[39m     \u001B[38;5;28mself\u001B[39m.command_header +\\\n\u001B[32m   1318\u001B[39m     args_command +\\\n\u001B[32m   1319\u001B[39m     proto.END_COMMAND_PART\n\u001B[32m   1321\u001B[39m answer = \u001B[38;5;28mself\u001B[39m.gateway_client.send_command(command)\n\u001B[32m-> \u001B[39m\u001B[32m1322\u001B[39m return_value = \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1323\u001B[39m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1325\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[32m   1326\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[33m\"\u001B[39m\u001B[33m_detach\u001B[39m\u001B[33m\"\u001B[39m):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/data4_project_group5/myenv/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:185\u001B[39m, in \u001B[36mcapture_sql_exception.<locals>.deco\u001B[39m\u001B[34m(*a, **kw)\u001B[39m\n\u001B[32m    181\u001B[39m converted = convert_exception(e.java_exception)\n\u001B[32m    182\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[32m    183\u001B[39m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[32m    184\u001B[39m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m185\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    186\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    187\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "\u001B[31mAnalysisException\u001B[39m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `wd`.`weather_time` cannot be resolved. Did you mean one of the following? [`wd`.`weather`, `dd`.`date_sk`, `src`.`endtime`, `du`.`userid`, `wd`.`base`].; line 25 pos 67;\n'Project ['src.rideid AS ride_id#2867, 'du.userSK AS user_sk#2868, 'src.startlockid AS start_lock_id#2869, 'src.endlockid AS end_lock_id#2870, 'dd.date_sk AS date_sk#2871, 'dv.vehicleid AS vehicle_id#2872, ('src.endtime - 'src.starttime) AS ride_duration#2873, 'haversine_km(cast('SPLIT('REPLACE('dl_start.gpscoord, (, ), ,)[0] as double), cast('SPLIT('REPLACE('REPLACE('dl_start.gpscoord, ), ), (, ), ,)[1] as double), cast('SPLIT('REPLACE('dl_end.gpscoord, (, ), ,)[0] as double), cast('SPLIT('REPLACE('REPLACE('dl_end.gpscoord, ), ), (, ), ,)[1] as double)) AS distance_km#2874, 'dw.weather_id, 'md5('concat('src.rideid, 'du.userSK, 'src.startlockid, 'src.endlockid, 'dd.date_sk, 'dv.vehicleid)) AS md5#2875]\n+- 'Join LeftOuter, ('wd.weather_condition = 'dw.weather_condition)\n   :- 'Join LeftOuter, ((zipcode#1289 = zipcode#863) AND (date_trunc(hour, starttime#840, Some(Europe/Brussels)) = 'date_trunc(hour, 'wd.weather_time)))\n   :  :- Join LeftOuter, (vehicleid#842 = vehicleid#1218)\n   :  :  :- Join LeftOuter, (cast(starttime#840 as date) = date#1201)\n   :  :  :  :- Join LeftOuter, (endlockid#845 = lockid#2876)\n   :  :  :  :  :- Join LeftOuter, (startlockid#844 = lockid#1284)\n   :  :  :  :  :  :- Join LeftOuter, (userid#858 = cast(userSK#1224 as int))\n   :  :  :  :  :  :  :- Join LeftOuter, (subscriptionid#843 = subscriptionid#855)\n   :  :  :  :  :  :  :  :- SubqueryAlias src\n   :  :  :  :  :  :  :  :  +- SubqueryAlias rides_source\n   :  :  :  :  :  :  :  :     +- View (`rides_source`, [rideid#837,startpoint#838,endpoint#839,starttime#840,endtime#841,vehicleid#842,subscriptionid#843,startlockid#844,endlockid#845])\n   :  :  :  :  :  :  :  :        +- Relation [rideid#837,startpoint#838,endpoint#839,starttime#840,endtime#841,vehicleid#842,subscriptionid#843,startlockid#844,endlockid#845] JDBCRelation(rides) [numPartitions=4]\n   :  :  :  :  :  :  :  +- SubqueryAlias sub\n   :  :  :  :  :  :  :     +- SubqueryAlias subscriptions_source\n   :  :  :  :  :  :  :        +- View (`subscriptions_source`, [subscriptionid#855,validfrom#856,subscriptiontypeid#857,userid#858])\n   :  :  :  :  :  :  :           +- Relation [subscriptionid#855,validfrom#856,subscriptiontypeid#857,userid#858] JDBCRelation(subscriptions) [numPartitions=4]\n   :  :  :  :  :  :  +- SubqueryAlias du\n   :  :  :  :  :  :     +- SubqueryAlias dimuser\n   :  :  :  :  :  :        +- View (`dimUser`, [userSK#1224,userid#1225,name#1226,email#1227,street#1228,number#1229,zipcode#1230,city#1231,country_code#1232,scd_start#1233,scd_end#1234,md5#1235,current#1236])\n   :  :  :  :  :  :           +- Relation [userSK#1224,userid#1225,name#1226,email#1227,street#1228,number#1229,zipcode#1230,city#1231,country_code#1232,scd_start#1233,scd_end#1234,md5#1235,current#1236] parquet\n   :  :  :  :  :  +- SubqueryAlias dl_start\n   :  :  :  :  :     +- SubqueryAlias dimlock\n   :  :  :  :  :        +- View (`dimLock`, [lockid#1284,stationid#1285,stationnr#1286,street#1287,number#1288,zipcode#1289,district#1290,gpscoord#1291])\n   :  :  :  :  :           +- Relation [lockid#1284,stationid#1285,stationnr#1286,street#1287,number#1288,zipcode#1289,district#1290,gpscoord#1291] parquet\n   :  :  :  :  +- SubqueryAlias dl_end\n   :  :  :  :     +- SubqueryAlias dimlock\n   :  :  :  :        +- View (`dimLock`, [lockid#2876,stationid#2877,stationnr#2878,street#2879,number#2880,zipcode#2881,district#2882,gpscoord#2883])\n   :  :  :  :           +- Relation [lockid#2876,stationid#2877,stationnr#2878,street#2879,number#2880,zipcode#2881,district#2882,gpscoord#2883] parquet\n   :  :  :  +- SubqueryAlias dd\n   :  :  :     +- SubqueryAlias dimdate\n   :  :  :        +- View (`dimDate`, [date_sk#1200L,date#1201,year#1202,quarter#1203,month_nr#1204,month_name#1205,day_nr#1206,day_name#1207,is_weekday#1208])\n   :  :  :           +- Relation [date_sk#1200L,date#1201,year#1202,quarter#1203,month_nr#1204,month_name#1205,day_nr#1206,day_name#1207,is_weekday#1208] parquet\n   :  :  +- SubqueryAlias dv\n   :  :     +- SubqueryAlias dimvehicle\n   :  :        +- View (`dimVehicle`, [vehicleid#1218,biketypeid#1219,biketypedescription#1220])\n   :  :           +- Relation [vehicleid#1218,biketypeid#1219,biketypedescription#1220] parquet\n   :  +- SubqueryAlias wd\n   :     +- SubqueryAlias weather_data\n   :        +- View (`weather_data`, [zipCode#863,coord#864,weather#865,base#866,main#867,visibility#868,wind#869,rain#870,clouds#871,dt#872,sys#873,timezone#874,id#875,name#876,cod#877])\n   :           +- Relation [zipCode#863,coord#864,weather#865,base#866,main#867,visibility#868,wind#869,rain#870,clouds#871,dt#872,sys#873,timezone#874,id#875,name#876,cod#877] json\n   +- SubqueryAlias dw\n      +- SubqueryAlias dimweather\n         +- View (`dimWeather`, [weather_id#1280,weather_condition#1281])\n            +- Relation [weather_id#1280,weather_condition#1281] parquet\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T16:42:31.052624Z",
     "start_time": "2025-03-29T16:42:31.017493Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ride_src_df.createOrReplaceTempView(\"rides_source\")\n",
    "subscriptions_df.createOrReplaceTempView(\"subscriptions_source\")"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T16:12:00.237604Z",
     "start_time": "2025-03-29T16:11:57.893069Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# trying to fix null subscription ids \n",
    "xd = spark.sql(\"\"\"\n",
    "SELECT src.subscriptionid \n",
    "FROM rides_source AS src\n",
    "LEFT JOIN subscriptions_source AS sub ON sub.subscriptionid = src.subscriptionid\n",
    "WHERE sub.subscriptionid IS NULL;\n",
    "\n",
    "\"\"\")\n",
    "xd.show()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 204:==========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|subscriptionid|\n",
      "+--------------+\n",
      "|          NULL|\n",
      "|          NULL|\n",
      "|          NULL|\n",
      "|          NULL|\n",
      "|          NULL|\n",
      "|          NULL|\n",
      "|          NULL|\n",
      "|          NULL|\n",
      "|          NULL|\n",
      "|          NULL|\n",
      "|          NULL|\n",
      "|          NULL|\n",
      "|          NULL|\n",
      "|          NULL|\n",
      "|          NULL|\n",
      "|          NULL|\n",
      "|          NULL|\n",
      "|          NULL|\n",
      "|          NULL|\n",
      "|          NULL|\n",
      "+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-29T16:43:09.386159Z",
     "start_time": "2025-03-29T16:42:37.173923Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ridesFactFromSource.printSchema()\n",
    "ridesFactFromSource.show(20)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ride_id: integer (nullable = true)\n",
      " |-- user_sk: string (nullable = true)\n",
      " |-- start_lock_id: integer (nullable = true)\n",
      " |-- end_lock_id: integer (nullable = true)\n",
      " |-- date_sk: long (nullable = true)\n",
      " |-- vehicle_id: integer (nullable = true)\n",
      " |-- ride_duration: interval day to second (nullable = true)\n",
      " |-- distance_km: double (nullable = true)\n",
      " |-- md5: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/29 17:42:37 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 52:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------------+-----------+-------+----------+--------------------+------------------+----+\n",
      "|ride_id|user_sk|start_lock_id|end_lock_id|date_sk|vehicle_id|       ride_duration|       distance_km| md5|\n",
      "+-------+-------+-------------+-----------+-------+----------+--------------------+------------------+----+\n",
      "|      3|   NULL|         2046|       1951|   2455|      NULL|INTERVAL '-1095 0...|1.4949081239108692|NULL|\n",
      "|     11|   NULL|          985|       2148|   2455|      NULL|INTERVAL '-1095 0...| 2.067612187914813|NULL|\n",
      "|     13|   NULL|         5619|       2717|   2455|      NULL|INTERVAL '-1095 0...| 5.381957169382341|NULL|\n",
      "|     17|   NULL|         2046|       1951|   3916|      NULL|INTERVAL '0 00:02...|1.4949081239108692|NULL|\n",
      "|      1|   NULL|         4849|       3188|   2455|      NULL|INTERVAL '-1095 0...| 3.491018931475966|NULL|\n",
      "|      4|   NULL|         1821|       2186|   2455|      NULL|INTERVAL '-1095 0...|2.3114308809290756|NULL|\n",
      "|      5|   NULL|         6382|       2700|   2455|      NULL|INTERVAL '-1095 0...|   5.5271668137006|NULL|\n",
      "|      9|   NULL|           50|       2067|   2455|      NULL|INTERVAL '-1095 0...|1.2010324044122915|NULL|\n",
      "|     10|   NULL|         NULL|       NULL|   2455|      NULL|INTERVAL '-1095 0...|              NULL|NULL|\n",
      "|     14|   NULL|         3531|       3554|   2455|      NULL|INTERVAL '-1095 0...|               0.0|NULL|\n",
      "|     15|   NULL|         4849|       3188|   3916|      NULL|INTERVAL '0 00:14...| 3.491018931475966|NULL|\n",
      "|     18|   NULL|         1821|       2186|   3916|      NULL|INTERVAL '0 00:05...|2.3114308809290756|NULL|\n",
      "|     19|   NULL|         6382|       2700|   3916|      NULL|INTERVAL '0 00:18...|   5.5271668137006|NULL|\n",
      "|      2|   NULL|         NULL|       NULL|   2455|      NULL|INTERVAL '-1095 0...|              NULL|NULL|\n",
      "|      6|   NULL|         NULL|       NULL|   2455|      NULL|INTERVAL '-1095 0...|              NULL|NULL|\n",
      "|      7|   NULL|         1388|       3401|   2455|      NULL|INTERVAL '-1095 0...|  7.07224625834132|NULL|\n",
      "|      8|   NULL|         2572|         13|   2455|      NULL|INTERVAL '-1095 0...|1.1373196748870393|NULL|\n",
      "|     12|   NULL|         2039|       3038|   2455|      NULL|INTERVAL '-1095 0...| 3.050318586470677|NULL|\n",
      "|     16|   NULL|         NULL|       NULL|   3916|      NULL|INTERVAL '0 00:02...|              NULL|NULL|\n",
      "|     20|   NULL|         NULL|       NULL|   3916|      NULL|INTERVAL '0 00:01...|              NULL|NULL|\n",
      "+-------+-------+-------------+-----------+-------+----------+--------------------+------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Initial load\n",
    "The first time loading the fact table perform a FULL load. All data is written to the Delta Table.\n",
    "After initial load the code line has to be disabled"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "\n",
    "ridesFactFromSource.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"factRides\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-10T09:49:24.830124800Z",
     "start_time": "2024-09-10T09:49:10.188063700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Incremental load\n",
    "When previous runs where performend you can opt for a 'faster' incremental run that only writes away changes. UPDATES and INSERTS are performed in one run.\n",
    "In our solution we use an md5 based on all fields in the source table to detect changes. This is not the most efficient way to detect changes. A better way is to use a timestamp field in the source table and use that to detect changes. This is not implemented in this example."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------------+----------------+-----------------+\n",
      "|num_affected_rows|num_updated_rows|num_deleted_rows|num_inserted_rows|\n",
      "+-----------------+----------------+----------------+-----------------+\n",
      "|                0|               0|               0|                0|\n",
      "+-----------------+----------------+----------------+-----------------+\n"
     ]
    }
   ],
   "source": [
    "dt_factRides = DeltaTable.forPath(spark,\".\\spark-warehouse\\\\factrides\")\n",
    "dt_factRides.toDF().createOrReplaceTempView(\"factRides_current\")\n",
    "#Merge to perform updates (TODO: Implement md5 strategy)\n",
    "\n",
    "result = spark.sql(\"MERGE INTO factRides_current AS target \\\n",
    "      using factRides_new AS source ON target.rideID = source.rideID \\\n",
    "      WHEN MATCHED and source.MD5<>target.MD5 THEN UPDATE SET * \\\n",
    "      WHEN NOT MATCHED THEN INSERT *\")\n",
    "\n",
    "result.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-10T09:49:36.248336800Z",
     "start_time": "2024-09-10T09:49:24.834540600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|     999|\n",
      "+--------+\n",
      "\n",
      "+-------+------+--------------------+--------+----------+--------------------+\n",
      "|OrderID|dateSK|          salesrepSK|count_mv|revenue_mv|                 md5|\n",
      "+-------+------+--------------------+--------+----------+--------------------+\n",
      "|      1|   650|b65df3d9-20dc-42d...|       1| 851804379|a237b06f2932af7dd...|\n",
      "+-------+------+--------------------+--------+----------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "# IMPORTANT: ALWAYS TEST THE CREATED CODE.\n",
    "# In this example I changed order 498 in the operational database and checked the change after the run.\n",
    "# spark.sql(\"select * from factsales f join dimsalesrep ds on f.salesrepSK = ds.salesrepSK where OrderID = 192  \").show()\n",
    "spark.sql(\"select count(*) from factrides\").show()\n",
    "spark.sql(\"select * from factrides where rideId=1\").show()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-10T09:49:41.411568200Z",
     "start_time": "2024-09-10T09:49:36.262239400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Checking the history of your delta fact table"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fact' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# The history information is derived from the delta table log files. They contain a lot of information of all the actions performed on the table. In this case it tells us something about de merge operations. You can find statistics about the update and insert counts in the document.\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m \u001B[43mfact\u001B[49m\u001B[38;5;241m.\u001B[39mhistory()\u001B[38;5;241m.\u001B[39mshow(\u001B[38;5;241m10\u001B[39m,\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'fact' is not defined"
     ]
    }
   ],
   "source": [
    "# The history information is derived from the delta table log files. They contain a lot of information of all the actions performed on the table. In this case it tells us something about de merge operations. You can find statistics about the update and insert counts in the document.\n",
    "\n",
    "fact.history().show(10,False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-10T09:49:42.067046800Z",
     "start_time": "2024-09-10T09:49:41.415008600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "spark.stop()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-19T11:02:02.240620Z",
     "start_time": "2025-03-19T11:01:55.677763Z"
    }
   },
   "outputs": [],
   "execution_count": 13
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
