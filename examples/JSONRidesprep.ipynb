{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-09T15:13:16.192546Z",
     "start_time": "2025-05-09T15:13:15.991019Z"
    }
   },
   "source": [
    "import ConnectionConfigKaloyan as cc\n",
    "from delta import DeltaTable\n",
    "cc.setupEnvironment()"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T15:17:24.318427Z",
     "start_time": "2025-05-09T15:16:37.312603Z"
    }
   },
   "cell_type": "code",
   "source": [
    "spark = cc.startLocalCluster(\"JSON_PREP\")\n",
    "spark.getActiveSession()"
   ],
   "id": "d31e7e787a1ee5a7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x15188b43ad0>"
      ],
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-DJQSI3S:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>JSON_PREP</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T15:17:34.306235Z",
     "start_time": "2025-05-09T15:17:29.313561Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cc.set_connectionProfile(\"default\")\n",
    "ride_src_df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", cc.create_jdbc()) \\\n",
    "    .option(\"driver\" , cc.get_Property(\"driver\")) \\\n",
    "    .option(\"dbtable\",\"rides\").option(\"user\", cc.get_Property(\"username\")) \\\n",
    "    .option(\"password\", cc.get_Property(\"password\")) \\\n",
    "    .option(\"partitionColumn\", \"rideid\") \\\n",
    "    .option(\"numPartitions\", 4) \\\n",
    "    .option(\"lowerBound\", 0) \\\n",
    "    .option(\"upperBound\", 1000) \\\n",
    "    .load()"
   ],
   "id": "dec72265aa816490",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T15:17:54.653154Z",
     "start_time": "2025-05-09T15:17:45.405222Z"
    }
   },
   "cell_type": "code",
   "source": [
    "selected_rides_df = ride_src_df.select(\"rideid\", \"starttime\", \"endtime\",\"startpoint\", \"endpoint\",\"vehicleid\")\n",
    "selected_rides_df.show(5)"
   ],
   "id": "c09aa6d7b31ad922",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+-------------------+-----------------+-----------------+---------+\n",
      "|rideid|          starttime|            endtime|       startpoint|         endpoint|vehicleid|\n",
      "+------+-------------------+-------------------+-----------------+-----------------+---------+\n",
      "|     1|2015-09-22 00:00:00|2012-09-22 00:00:00|(51.2083,4.44595)|(51.1938,4.40228)|      844|\n",
      "|     2|2015-09-22 00:00:00|2012-09-22 00:00:00|(51.2174,4.41597)|(51.2188,4.40935)|     4545|\n",
      "|     3|2015-09-22 00:00:00|2012-09-22 00:00:00|(51.2088,4.40834)|(51.2077,4.39846)|     3419|\n",
      "|     4|2015-09-22 00:00:00|2012-09-22 00:00:00|(51.2023,4.41208)|(51.2119,4.39894)|     1208|\n",
      "|     5|2015-09-22 00:00:00|2012-09-22 00:00:00|(51.1888,4.45039)|(51.2221,4.40467)|     5536|\n",
      "+------+-------------------+-------------------+-----------------+-----------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T15:17:59.371511Z",
     "start_time": "2025-05-09T15:17:59.257259Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Filter by time interval (e.g., first week of July 2023)\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "filtered_rides_df = selected_rides_df.filter(\n",
    "    (col(\"starttime\") >= \"2023-07-01\") & (col(\"starttime\") < \"2023-07-07\")\n",
    ")"
   ],
   "id": "a0ad8d0bd65d3ce3",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T15:20:45.188365Z",
     "start_time": "2025-05-09T15:20:44.863433Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load vehicle info\n",
    "vehicle_df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", cc.create_jdbc()) \\\n",
    "    .option(\"driver\" , cc.get_Property(\"driver\")) \\\n",
    "    .option(\"dbtable\",\"vehicles\").option(\"user\", cc.get_Property(\"username\")) \\\n",
    "    .option(\"password\", cc.get_Property(\"password\")) \\\n",
    "    .option(\"partitionColumn\", \"vehicleid\") \\\n",
    "    .option(\"numPartitions\", 4) \\\n",
    "    .option(\"lowerBound\", 0) \\\n",
    "    .option(\"upperBound\", 1000) \\\n",
    "    .load()"
   ],
   "id": "e2e8d486a514926b",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T15:21:02.862323Z",
     "start_time": "2025-05-09T15:21:02.736334Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Join rides with vehicle info\n",
    "rides_with_vehicle = filtered_rides_df.join(vehicle_df, on=\"vehicleid\", how=\"left\")"
   ],
   "id": "1ef3766cb22e7996",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T15:26:33.140726Z",
     "start_time": "2025-05-09T15:26:33.028772Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 3: Nest vehicle fields using struct\n",
    "from pyspark.sql.functions import col, struct\n",
    "rides_nested = rides_with_vehicle.select(\n",
    "    col(\"rideid\"),\n",
    "    col(\"starttime\"),\n",
    "    col(\"endtime\"),\n",
    "    col(\"startpoint\"),\n",
    "    col(\"endpoint\"),\n",
    "    struct(\n",
    "        col(\"vehicleid\"),\n",
    "        col(\"serialnumber\"),\n",
    "        col(\"lastmaintenanceon\"),\n",
    "        col(\"position\")\n",
    "    ).alias(\"vehicle\")\n",
    ")"
   ],
   "id": "f3b8d0d7865e6228",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T15:27:56.941193Z",
     "start_time": "2025-05-09T15:27:50.450794Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Write to JSON for MongoDB import\n",
    "rides_nested.write.mode(\"overwrite\").json(\"rides_json_output\")"
   ],
   "id": "404dc70833f89983",
   "outputs": [],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
